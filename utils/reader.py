import json
import os
import random
import sys
from typing import List

import librosa
import numpy as np
import soundfile
from torch.utils.data import Dataset
from tqdm import tqdm

from utils.binary import DatasetReader


class CustomDataset(Dataset):
    def __init__(self,
                 data_list_path,
                 processor,
                 mono=True,
                 language=None,
                 timestamps=False,
                 sample_rate=16000,
                 min_duration=0.5,
                 max_duration=30,
                 min_sentence=1,
                 max_sentence=200,
                 augment_config_path=None):
        """
        Args:
            data_list_path: æ•°æ®åˆ—è¡¨æ–‡ä»¶çš„è·¯å¾„ï¼Œæˆ–è€…äºŒè¿›åˆ¶åˆ—è¡¨çš„å¤´æ–‡ä»¶è·¯å¾„
            processor: Whisperçš„é¢„å¤„ç†å·¥å…·ï¼ŒWhisperProcessor.from_pretrainedè·å–
            mono: æ˜¯å¦å°†éŸ³é¢‘è½¬æ¢æˆå•é€šé“ï¼Œè¿™ä¸ªå¿…é¡»æ˜¯True
            language: å¾®è°ƒæ•°æ®çš„è¯­è¨€
            timestamps: å¾®è°ƒæ—¶æ˜¯å¦ä½¿ç”¨æ—¶é—´æˆ³
            sample_rate: éŸ³é¢‘çš„é‡‡æ ·ç‡ï¼Œé»˜è®¤æ˜¯16000
            min_duration: å°äºè¿™ä¸ªæ—¶é—´æ®µçš„éŸ³é¢‘å°†è¢«æˆªæ–­ï¼Œå•ä½ç§’ï¼Œä¸èƒ½å°äº0.5ï¼Œé»˜è®¤0.5s
            max_duration: å¤§äºè¿™ä¸ªæ—¶é—´æ®µçš„éŸ³é¢‘å°†è¢«æˆªæ–­ï¼Œå•ä½ç§’ï¼Œä¸èƒ½å¤§äº30ï¼Œé»˜è®¤30s
            min_sentence: å¾®è°ƒæ—¶æœ€å°‘çš„å¥å­å­—æ•°ï¼Œé»˜è®¤1
            max_sentence: å¾®è°ƒæ—¶æœ€å¤šå¥å­å­—æ•°ï¼Œé»˜è®¤200
            augment_config_path: æ•°æ®å¢å¼ºé…ç½®å‚æ•°æ–‡ä»¶è·¯å¾„
        """
        super(CustomDataset, self).__init__()
        assert min_duration >= 0.5, f"min_durationä¸èƒ½å°äº0.5ï¼Œå½“å‰ä¸ºï¼š{min_duration}"
        assert max_duration <= 30, f"max_durationä¸èƒ½å¤§äº30ï¼Œå½“å‰ä¸ºï¼š{max_duration}"
        assert min_sentence >= 1, f"min_sentenceä¸èƒ½å°äº1ï¼Œå½“å‰ä¸ºï¼š{min_sentence}"
        assert max_sentence <= 200, f"max_sentenceä¸èƒ½å¤§äº200ï¼Œå½“å‰ä¸ºï¼š{max_sentence}"
        self.data_list_path = data_list_path
        self.processor = processor
        self.data_list_path = data_list_path
        self.sample_rate = sample_rate
        self.mono = mono
        self.language = language
        self.timestamps = timestamps
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.min_sentence = min_sentence
        self.max_sentence = max_sentence
        self.vocab = self.processor.tokenizer.get_vocab()
        self.startoftranscript = self.vocab['<|startoftranscript|>']
        self.endoftext = self.vocab['<|endoftext|>']
        if '<|nospeech|>' in self.vocab.keys():
            self.nospeech = self.vocab['<|nospeech|>']
            self.timestamp_begin = None
        else:
            # å…¼å®¹æ—§æ¨¡å‹
            self.nospeech = self.vocab['<|nocaptions|>']
            self.timestamp_begin = self.vocab['<|notimestamps|>'] + 1
        self.data_list: List[dict] = []
        # åŠ è½½æ•°æ®åˆ—è¡¨
        self._load_data_list()
        # é è¦½èªè¨€åˆ†ä½ˆ
        self._preview_language_distribution()
        # è¨­ç½®è‡ªå®šç¾©èªè¨€ token
        self._setup_custom_language_tokens()
        # æ•°æ®å¢å¼ºé…ç½®å‚æ•°
        self.augment_configs = None
        self.noises_path = None
        self.speed_rates = None
        if augment_config_path:
            with open(augment_config_path, 'r', encoding='utf-8') as f:
                self.augment_configs = json.load(f)

    # åŠ è½½æ•°æ®åˆ—è¡¨
    def _load_data_list(self):
        if self.data_list_path.endswith(".header"):
            # è·å–äºŒè¿›åˆ¶çš„æ•°æ®åˆ—è¡¨
            self.dataset_reader = DatasetReader(data_header_path=self.data_list_path,
                                                min_duration=self.min_duration,
                                                max_duration=self.max_duration)
            self.data_list = self.dataset_reader.get_keys()
        else:
            # è·å–æ•°æ®åˆ—è¡¨
            with open(self.data_list_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            self.data_list = []
            for line in tqdm(lines, desc='è¯»å–æ•°æ®åˆ—è¡¨'):
                if isinstance(line, str):
                    line = json.loads(line)
                if not isinstance(line, dict): continue
                # è·³è¿‡è¶…å‡ºé•¿åº¦é™åˆ¶çš„éŸ³é¢‘
                if line["duration"] < self.min_duration:
                    continue
                if self.max_duration != -1 and line["duration"] > self.max_duration:
                    continue
                # è·³è¿‡è¶…å‡ºå¥å­å­—æ•°é™åˆ¶çš„éŸ³é¢‘
                if 'sentence' in line.keys():
                    if len(line["sentence"]) < self.min_sentence or len(line["sentence"]) > self.max_sentence:
                        continue
                else:
                    sentence_len = 0
                    for s in line["sentences"]:
                        sentence_len += len(s['text'])
                    if sentence_len < self.min_sentence or sentence_len > self.max_sentence:
                        continue
                self.data_list.append(dict(line))

    # ä»æ•°æ®åˆ—è¡¨é‡Œé¢è·å–éŸ³é¢‘æ•°æ®ã€é‡‡æ ·ç‡å’Œæ–‡æœ¬
    def _get_list_data(self, idx):
        if self.data_list_path.endswith(".header"):
            data_list = self.dataset_reader.get_data(self.data_list[idx])
        else:
            data_list = self.data_list[idx]
        # åˆ†å‰²éŸ³é¢‘è·¯å¾„å’Œæ ‡ç­¾
        audio_file = data_list["audio"]['path']
        transcript = data_list["sentences"] if self.timestamps else data_list["sentence"]
        language = data_list["language"] if 'language' in data_list.keys() else None
        if 'start_time' not in data_list["audio"].keys():
            sample, sample_rate = soundfile.read(audio_file, dtype='float32')
        else:
            start_time, end_time = data_list["audio"]["start_time"], data_list["audio"]["end_time"]
            # åˆ†å‰²è¯»å–éŸ³é¢‘
            sample, sample_rate = self.slice_from_file(audio_file, start=start_time, end=end_time)
        sample = sample.T
        # è½¬æˆå•é€šé“
        if self.mono:
            sample = librosa.to_mono(sample)
        # æ•°æ®å¢å¼º
        if self.augment_configs:
            sample, sample_rate = self.augment(sample, sample_rate)
        # é‡é‡‡æ ·
        if self.sample_rate != sample_rate:
            sample = self.resample(sample, orig_sr=sample_rate, target_sr=self.sample_rate)
        return sample, sample_rate, transcript, language

    def _load_timestamps_transcript(self, transcript: List[dict]):
        assert isinstance(transcript, list), f"transcriptåº”è¯¥ä¸ºlistï¼Œå½“å‰ä¸ºï¼š{type(transcript)}"
        data = dict()
        labels = self.processor.tokenizer.prefix_tokens[:3]
        for t in transcript:
            # å°†ç›®æ ‡æ–‡æœ¬ç¼–ç ä¸ºæ ‡ç­¾ID
            start = t['start'] if round(t['start'] * 100) % 2 == 0 else t['start'] + 0.01
            if self.timestamp_begin is None:
                start = self.vocab[f'<|{start:.2f}|>']
            else:
                start = self.timestamp_begin + round(start * 100) // 2
            end = t['end'] if round(t['end'] * 100) % 2 == 0 else t['end'] - 0.01
            if self.timestamp_begin is None:
                end = self.vocab[f'<|{end:.2f}|>']
            else:
                end = self.timestamp_begin + round(end * 100) // 2
            label = self.processor(text=t['text']).input_ids[4:-1]
            labels.extend([start])
            labels.extend(label)
            labels.extend([end])
        data['labels'] = labels + [self.endoftext]
        return data

    def _preview_language_distribution(self):
        """
        é è¦½è³‡æ–™é›†ä¸­çš„èªè¨€åˆ†ä½ˆæƒ…æ³
        çµ±è¨ˆæ¯ç¨®èªè¨€çš„æ¨£æœ¬æ•¸é‡å’Œç¸½æ™‚é•·
        """
        language_stats = {}
        total_samples = 0
        total_duration = 0.0
        
        print("\n" + "="*60)
        print("ğŸ“Š è³‡æ–™é›†èªè¨€åˆ†ä½ˆé è¦½")
        print("="*60)
        
        for data in self.data_list:
            # ç²å–èªè¨€æ¨™ç±¤ï¼ˆå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨å…¨åŸŸè¨­å®šï¼‰
            language = data.get('language', self.language)
            if language is None:
                language = 'Unknown'
            
            # æ¨™æº–åŒ–èªè¨€åç¨±ï¼ˆè½‰å°å¯«ï¼‰
            language = language.lower()
            
            # çµ±è¨ˆ
            if language not in language_stats:
                language_stats[language] = {
                    'count': 0,
                    'duration': 0.0
                }
            
            language_stats[language]['count'] += 1
            language_stats[language]['duration'] += data.get('duration', 0.0)
            
            total_samples += 1
            total_duration += data.get('duration', 0.0)
        
        # é¡¯ç¤ºçµ±è¨ˆçµæœ
        print(f"ğŸ“‹ ç¸½æ¨£æœ¬æ•¸ï¼š{total_samples:,}")
        print(f"â±ï¸  ç¸½æ™‚é•·ï¼š{total_duration:.2f} å°æ™‚ ({total_duration*60:.1f} åˆ†é˜)")
        print(f"ğŸŒ èªè¨€ç¨®é¡ï¼š{len(language_stats)} ç¨®")
        print("\nğŸ“ˆ å„èªè¨€è©³ç´°çµ±è¨ˆï¼š")
        print("-" * 60)
        print(f"{'èªè¨€':<20} {'æ¨£æœ¬æ•¸':<10} {'æ™‚é•·(å°æ™‚)':<12} {'ç™¾åˆ†æ¯”':<8}")
        print("-" * 60)
        
        # æŒ‰æ¨£æœ¬æ•¸æ’åºé¡¯ç¤º
        sorted_languages = sorted(language_stats.items(), 
                                key=lambda x: x[1]['count'], 
                                reverse=True)
        
        for language, stats in sorted_languages:
            count = stats['count']
            duration = stats['duration']
            percentage = (count / total_samples) * 100
            
            # ç‚ºå®¢å®¶è©±è…”èª¿æ·»åŠ ç‰¹æ®Šæ¨™è¨˜
            display_name = language
            if language.startswith('hakka_'):
                display_name = f"ğŸ—£ï¸  {language}"
            
            print(f"{display_name:<20} {count:<10,} {duration:<12.2f} {percentage:<8.1f}%")
        
        print("-" * 60)
        print()

    def _setup_custom_language_tokens(self):
        """
        çœŸæ­£æ·»åŠ å®¢å®¶è©±èªè¨€ token åˆ° Whisper tokenizer çš„è©å½™è¡¨
        æ“´å±•èªè¨€æ”¯æ´è€Œä¸æ˜¯ç¹éé©—è­‰
        """
        hakka_languages = {
            'hakka_sixian': '<|hakka_sixian|>',
            'hakka_hailu': '<|hakka_hailu|>',
            'hakka_dapu': '<|hakka_dapu|>',
            'hakka_raoping': '<|hakka_raoping|>',
            'hakka_zhaoan': '<|hakka_zhaoan|>',
            'hakka_nansixian': '<|hakka_nansixian|>'
        }
        
        print(f"ğŸ”§ é–‹å§‹æ“´å±• Whisper tokenizer è©å½™è¡¨")
        print(f"ğŸ“‹ è¦æ·»åŠ çš„å®¢å®¶è©±è…”èª¿ï¼š{list(hakka_languages.keys())}")
        
        tokenizer = self.processor.tokenizer
        
        # 1. æ·»åŠ æ–°çš„èªè¨€ token åˆ°è©å½™è¡¨
        new_tokens = []
        vocab = tokenizer.get_vocab()
        
        for lang_code, token in hakka_languages.items():
            if token not in vocab:
                new_tokens.append(token)
                print(f"   â• æ·»åŠ èªè¨€ tokenï¼š{token}")
        
        if new_tokens:
            # æ·»åŠ ç‰¹æ®Š token
            tokenizer.add_special_tokens({"additional_special_tokens": new_tokens})
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_tokens)} å€‹èªè¨€ token")
            
            # é‡æ–°ç²å–æ›´æ–°å¾Œçš„è©å½™è¡¨
            updated_vocab = tokenizer.get_vocab()
            
            # 2. ä¿®è£œèªè¨€é©—è­‰é‚è¼¯ï¼Œè®“æ–°èªè¨€è¢«æ¥å—
            if hasattr(tokenizer, '_get_language_id'):
                original_get_language_id = tokenizer._get_language_id
                
                def patched_get_language_id(language):
                    if language and language.lower() in hakka_languages:
                        # è¿”å›å°æ‡‰çš„ token ID
                        token = hakka_languages[language.lower()]
                        token_id = updated_vocab.get(token)
                        print(f"ğŸ—£ï¸ å®¢å®¶è©±è…”èª¿ {language} -> token_id: {token_id}")
                        return token_id
                    else:
                        return original_get_language_id(language)
                
                tokenizer._get_language_id = patched_get_language_id
            
            # 3. ä¿®è£œ prefix_tokens å±¬æ€§ä¾†è™•ç†æ–°èªè¨€
            original_prefix_tokens_property = tokenizer.__class__.prefix_tokens
            
            def patched_prefix_tokens(self):
                # æª¢æŸ¥æ˜¯å¦æœ‰å®¢å®¶è©±èªè¨€è¨­å®š
                if hasattr(self, 'language') and self.language and self.language.lower() in hakka_languages:
                    lang_code = self.language.lower()
                    token = hakka_languages[lang_code]
                    token_id = updated_vocab.get(token)
                    
                    if token_id is not None:
                        # åªåœ¨ç¬¬ä¸€æ¬¡æˆ–æ¯1000æ¬¡æ™‚é¡¯ç¤ºï¼Œé¿å…åˆ·é »
                        if not hasattr(self, '_hakka_token_logged') or not hasattr(self, '_hakka_log_count'):
                            self._hakka_token_logged = set()
                            self._hakka_log_count = 0
                        
                        if token not in self._hakka_token_logged or self._hakka_log_count % 1000 == 0:
                            print(f"ğŸ¯ ä½¿ç”¨å®¢å®¶è©± tokenï¼š{token} (ID: {token_id})")
                            self._hakka_token_logged.add(token)
                        
                        self._hakka_log_count += 1
                        # æ§‹å»ºåŒ…å«å®¢å®¶è©±èªè¨€ token çš„å‰ç¶´
                        prefix_tokens = [
                            updated_vocab['<|startoftranscript|>'],
                            token_id,  # å®¢å®¶è©±èªè¨€ token
                            updated_vocab['<|transcribe|>'] if hasattr(self, 'task') and self.task == 'transcribe' else updated_vocab.get('<|translate|>', updated_vocab['<|transcribe|>'])
                        ]
                        return prefix_tokens
                
                # å…¶ä»–æƒ…æ³ä½¿ç”¨åŸå§‹é‚è¼¯
                return original_prefix_tokens_property.fget(self)
            
            # æ‡‰ç”¨ä¿®è£œ
            tokenizer.__class__.prefix_tokens = property(patched_prefix_tokens)
            
            print("ğŸ‰ å®¢å®¶è©±èªè¨€ token å·²æˆåŠŸæ•´åˆåˆ° Whisper tokenizer")
            print("âœ¨ ç¾åœ¨æ¯å€‹å®¢å®¶è©±è…”èª¿éƒ½æœ‰ç¨ç«‹çš„èªè¨€è­˜åˆ¥ token")
            
        else:
            print("â„¹ï¸  æ‰€æœ‰å®¢å®¶è©±èªè¨€ token å·²å­˜åœ¨ï¼Œç„¡éœ€æ·»åŠ ")
        
        return list(hakka_languages.keys())

    def _map_custom_language(self, language):
        """
        çµ±ä¸€è½‰æ›èªè¨€æ¨™ç±¤ç‚ºå°å¯«ï¼Œè®“æ¨¡å‹å­¸ç¿’å€åˆ†ä¸åŒçš„å®¢å®¶è©±è…”èª¿
        """
        if language is None:
            return None
        
        # å®¢å®¶è©±è®Šé«”åˆ—è¡¨
        hakka_variants = [
            'hakka_sixian', 'hakka_hailu', 'hakka_dapu', 
            'hakka_raoping', 'hakka_zhaoan', 'hakka_nansixian'
        ]
        
        # çµ±ä¸€è½‰å°å¯«
        language_lower = language.lower()
        
        if language_lower in hakka_variants:
            # è¿”å›å°å¯«çš„å®¢å®¶è©±æ¨™ç±¤
            return language_lower
        
        # å¦‚æœæ˜¯ Whisper æ”¯æ´çš„èªè¨€ï¼Œä¹Ÿè½‰å°å¯«
        return language_lower

    def __getitem__(self, idx):
        try:
            # ä»æ•°æ®åˆ—è¡¨é‡Œé¢è·å–éŸ³é¢‘æ•°æ®ã€é‡‡æ ·ç‡å’Œæ–‡æœ¬
            sample, sample_rate, transcript, language = self._get_list_data(idx=idx)
            # å¯ä»¥ä¸ºå•ç‹¬æ•°æ®è®¾ç½®è¯­è¨€
            # æ˜ å°„è‡ªå®šç¾©èªè¨€åˆ°æ”¯æ´çš„èªè¨€
            mapped_language = self._map_custom_language(language if language is not None else self.language)
            self.processor.tokenizer.set_prefix_tokens(language=mapped_language)
            if len(transcript) > 0:
                # åŠ è½½å¸¦æœ‰æ—¶é—´æˆ³çš„æ–‡æœ¬
                if self.timestamps:
                    data = self._load_timestamps_transcript(transcript=transcript)
                    # ä»è¾“å…¥éŸ³é¢‘æ•°ç»„ä¸­è®¡ç®—log-Melè¾“å…¥ç‰¹å¾
                    data["input_features"] = self.processor(audio=sample, sampling_rate=self.sample_rate).input_features
                else:
                    # è·å–log-Melç‰¹å¾å’Œæ ‡ç­¾ID
                    data = self.processor(audio=sample, sampling_rate=self.sample_rate, text=transcript)
            else:
                # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œåˆ™ä½¿ç”¨<|nospeech|>æ ‡è®°
                data = self.processor(audio=sample, sampling_rate=self.sample_rate)
                data['labels'] = [self.startoftranscript, self.nospeech, self.endoftext]
            return data
        except Exception as e:
            print(f'è¯»å–æ•°æ®å‡ºé”™ï¼Œåºå·ï¼š{idx}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}', file=sys.stderr)
            return self.__getitem__(random.randint(0, self.__len__() - 1))

    def __len__(self):
        return len(self.data_list)

    # åˆ†å‰²è¯»å–éŸ³é¢‘
    @staticmethod
    def slice_from_file(file, start, end):
        sndfile = soundfile.SoundFile(file)
        sample_rate = sndfile.samplerate
        duration = round(float(len(sndfile)) / sample_rate, 3)
        start = round(start, 3)
        end = round(end, 3)
        # ä»æœ«å°¾å¼€å§‹è®¡
        if start < 0.0: start += duration
        if end < 0.0: end += duration
        # ä¿è¯æ•°æ®ä¸è¶Šç•Œ
        if start < 0.0: start = 0.0
        if end > duration: end = duration
        if end < 0.0:
            raise ValueError("åˆ‡ç‰‡ç»“æŸä½ç½®(%f s)è¶Šç•Œ" % end)
        if start > end:
            raise ValueError("åˆ‡ç‰‡å¼€å§‹ä½ç½®(%f s)æ™šäºåˆ‡ç‰‡ç»“æŸä½ç½®(%f s)" % (start, end))
        start_frame = int(start * sample_rate)
        end_frame = int(end * sample_rate)
        sndfile.seek(start_frame)
        sample = sndfile.read(frames=end_frame - start_frame, dtype='float32')
        return sample, sample_rate

    # æ•°æ®å¢å¼º
    def augment(self, sample, sample_rate):
        for config in self.augment_configs:
            if config['type'] == 'speed' and random.random() < config['prob']:
                if self.speed_rates is None:
                    min_speed_rate, max_speed_rate, num_rates = config['params']['min_speed_rate'], \
                        config['params']['max_speed_rate'], config['params']['num_rates']
                    self.speed_rates = np.linspace(min_speed_rate, max_speed_rate, num_rates, endpoint=True)
                rate = random.choice(self.speed_rates)
                sample = self.change_speed(sample, speed_rate=rate)
            if config['type'] == 'shift' and random.random() < config['prob']:
                min_shift_ms, max_shift_ms = config['params']['min_shift_ms'], config['params']['max_shift_ms']
                shift_ms = random.randint(min_shift_ms, max_shift_ms)
                sample = self.shift(sample, sample_rate, shift_ms=shift_ms)
            if config['type'] == 'volume' and random.random() < config['prob']:
                min_gain_dBFS, max_gain_dBFS = config['params']['min_gain_dBFS'], config['params']['max_gain_dBFS']
                gain = random.randint(min_gain_dBFS, max_gain_dBFS)
                sample = self.volume(sample, gain=gain)
            if config['type'] == 'resample' and random.random() < config['prob']:
                new_sample_rates = config['params']['new_sample_rates']
                new_sample_rate = np.random.choice(new_sample_rates)
                sample = self.resample(sample, orig_sr=sample_rate, target_sr=new_sample_rate)
                sample_rate = new_sample_rate
            if config['type'] == 'noise' and random.random() < config['prob']:
                min_snr_dB, max_snr_dB = config['params']['min_snr_dB'], config['params']['max_snr_dB']
                if self.noises_path is None:
                    self.noises_path = []
                    noise_dir = config['params']['noise_dir']
                    if os.path.exists(noise_dir):
                        for file in os.listdir(noise_dir):
                            self.noises_path.append(os.path.join(noise_dir, file))
                noise_path = random.choice(self.noises_path)
                snr_dB = random.randint(min_snr_dB, max_snr_dB)
                sample = self.add_noise(sample, sample_rate, noise_path=noise_path, snr_dB=snr_dB)
        return sample, sample_rate

    # æ”¹å˜è¯­é€Ÿ
    @staticmethod
    def change_speed(sample, speed_rate):
        if speed_rate == 1.0:
            return sample
        if speed_rate <= 0:
            raise ValueError("é€Ÿåº¦é€Ÿç‡åº”å¤§äºé›¶")
        old_length = sample.shape[0]
        new_length = int(old_length / speed_rate)
        old_indices = np.arange(old_length)
        new_indices = np.linspace(start=0, stop=old_length, num=new_length)
        sample = np.interp(new_indices, old_indices, sample).astype(np.float32)
        return sample

    # éŸ³é¢‘åç§»
    @staticmethod
    def shift(sample, sample_rate, shift_ms):
        duration = sample.shape[0] / sample_rate
        if abs(shift_ms) / 1000.0 > duration:
            raise ValueError("shift_msçš„ç»å¯¹å€¼åº”è¯¥å°äºéŸ³é¢‘æŒç»­æ—¶é—´")
        shift_samples = int(shift_ms * sample_rate / 1000)
        if shift_samples > 0:
            sample[:-shift_samples] = sample[shift_samples:]
            sample[-shift_samples:] = 0
        elif shift_samples < 0:
            sample[-shift_samples:] = sample[:shift_samples]
            sample[:-shift_samples] = 0
        return sample

    # æ”¹å˜éŸ³é‡
    @staticmethod
    def volume(sample, gain):
        sample *= 10.**(gain / 20.)
        return sample

    # å£°éŸ³é‡é‡‡æ ·
    @staticmethod
    def resample(sample, orig_sr, target_sr):
        sample = librosa.resample(sample, orig_sr=orig_sr, target_sr=target_sr)
        return sample

    # æ·»åŠ å™ªå£°
    def add_noise(self, sample, sample_rate, noise_path, snr_dB, max_gain_db=300.0):
        noise_sample, sr = librosa.load(noise_path, sr=sample_rate)
        # æ ‡å‡†åŒ–éŸ³é¢‘éŸ³é‡ï¼Œä¿è¯å™ªå£°ä¸ä¼šå¤ªå¤§
        target_db = -20
        gain = min(max_gain_db, target_db - self.rms_db(sample))
        sample *= 10. ** (gain / 20.)
        # æŒ‡å®šå™ªå£°éŸ³é‡
        sample_rms_db, noise_rms_db = self.rms_db(sample), self.rms_db(noise_sample)
        noise_gain_db = min(sample_rms_db - noise_rms_db - snr_dB, max_gain_db)
        noise_sample *= 10. ** (noise_gain_db / 20.)
        # å›ºå®šå™ªå£°é•¿åº¦
        if noise_sample.shape[0] < sample.shape[0]:
            diff_duration = sample.shape[0] - noise_sample.shape[0]
            noise_sample = np.pad(noise_sample, (0, diff_duration), 'wrap')
        elif noise_sample.shape[0] > sample.shape[0]:
            start_frame = random.randint(0, noise_sample.shape[0] - sample.shape[0])
            noise_sample = noise_sample[start_frame:sample.shape[0] + start_frame]
        sample += noise_sample
        return sample

    @staticmethod
    def rms_db(sample):
        mean_square = np.mean(sample ** 2)
        return 10 * np.log10(mean_square)
        