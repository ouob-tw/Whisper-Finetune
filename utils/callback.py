import os
import shutil
import torch
import numpy as np
import json
from datetime import datetime
from torch.utils.data import DataLoader
from tqdm import tqdm
import evaluate

from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
from utils.data_utils import DataCollatorSpeechSeq2SeqWithPadding, remove_punctuation, to_simple


# ä¿å­˜æ¨¡å‹æ—¶çš„å›è°ƒå‡½æ•°
class SavePeftModelCallback(TrainerCallback):
    def on_save(self,
                args: TrainingArguments,
                state: TrainerState,
                control: TrainerControl,
                **kwargs, ):
        if args.local_rank == 0 or args.local_rank == -1:
            # ä¿å­˜æ•ˆæœæœ€å¥½çš„æ¨¡å‹
            best_checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-best")
            # å› ä¸ºåªä¿å­˜æœ€æ–°5ä¸ªæ£€æŸ¥ç‚¹ï¼Œæ‰€ä»¥è¦ç¡®ä¿ä¸æ˜¯ä¹‹å‰çš„æ£€æŸ¥ç‚¹
            if state.best_model_checkpoint is not None and os.path.exists(state.best_model_checkpoint):
                if os.path.exists(best_checkpoint_folder):
                    shutil.rmtree(best_checkpoint_folder)
                shutil.copytree(state.best_model_checkpoint, best_checkpoint_folder)
            print(f"æ•ˆæœæœ€å¥½çš„æ£€æŸ¥ç‚¹ä¸ºï¼š{state.best_model_checkpoint}ï¼Œè¯„ä¼°ç»“æœä¸ºï¼š{state.best_metric}")
        return control


# CER è©•ä¼°å›èª¿å‡½æ•¸
class CEREvaluationCallback(TrainerCallback):
    def __init__(self, eval_dataset, processor, eval_loss_threshold=0.3, remove_pun=True, to_simple=True):
        """
        Args:
            eval_dataset: è©•ä¼°æ•¸æ“šé›†
            processor: Whisper processor
            eval_loss_threshold: åªæœ‰ç•¶ eval_loss å°æ–¼æ­¤å€¼æ™‚æ‰é€²è¡Œ CER è©•ä¼°
            remove_pun: æ˜¯å¦ç§»é™¤æ¨™é»ç¬¦è™Ÿ
            to_simple: æ˜¯å¦è½‰æ›ç‚ºç°¡é«”ä¸­æ–‡
        """
        self.eval_dataset = eval_dataset
        self.processor = processor
        self.eval_loss_threshold = eval_loss_threshold
        self.remove_pun = remove_pun
        self.to_simple = to_simple
        self.data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
        self.cer_log_file = None  # å°‡åœ¨ on_train_begin ä¸­è¨­å®š
    
    def on_train_begin(self, args, state, control, **kwargs):
        """è¨“ç·´é–‹å§‹æ™‚è¨­ç½® CER æ—¥èªŒæ–‡ä»¶"""
        self.cer_log_file = os.path.join(args.output_dir, "cer_evaluation_log.json")
        
        # åˆå§‹åŒ– CER æ—¥èªŒæ–‡ä»¶
        initial_log = {
            "eval_loss_threshold": self.eval_loss_threshold,
            "remove_pun": self.remove_pun, 
            "to_simple": self.to_simple,
            "start_time": datetime.now().isoformat(),
            "evaluations": []
        }
        
        os.makedirs(args.output_dir, exist_ok=True)
        with open(self.cer_log_file, 'w', encoding='utf-8') as f:
            json.dump(initial_log, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ“ CER è©•ä¼°æ—¥èªŒå°‡ä¿å­˜åˆ°: {self.cer_log_file}")
        print(f"ğŸ¯ è©•ä¼°æ¢ä»¶: eval_loss < {self.eval_loss_threshold}")
        print(f"ğŸ“Š æ ¹æ“š eval_steps è¨­å®šï¼Œæ¯ {getattr(args, 'eval_steps', 'N/A')} æ­¥è©•ä¼°ä¸€æ¬¡")
        
    def on_evaluate(self, args, state, control, logs=None, model=None, **kwargs):
        """åœ¨æ¯æ¬¡è©•ä¼°å¾Œæª¢æŸ¥æ˜¯å¦éœ€è¦è¨ˆç®— CER"""
        if logs is None:
            return control
            
        eval_loss = logs.get('eval_loss', float('inf'))
        
        # åªæœ‰ç•¶ eval_loss å°æ–¼é–¾å€¼æ™‚æ‰é€²è¡Œ CER è©•ä¼°
        if eval_loss < self.eval_loss_threshold:
            print(f"\nğŸ¯ eval_loss ({eval_loss:.4f}) < {self.eval_loss_threshold}ï¼Œé–‹å§‹è¨ˆç®— CER...")
            
            try:
                cer_result = self._compute_cer(model, args)
                logs['eval_cer'] = cer_result
                print(f"ğŸ“Š CER çµæœ: {cer_result:.4f}")
                
                # ä¿å­˜ CER çµæœåˆ°æ—¥èªŒæ–‡ä»¶
                self._save_cer_result(state, eval_loss, cer_result)
                
                # å°‡ CER çµæœè¨˜éŒ„åˆ° state ä¸­ï¼ˆç”¨æ–¼æœ€ä½³æ¨¡å‹é¸æ“‡ï¼‰
                if not hasattr(state, 'cer_history'):
                    state.cer_history = []
                state.cer_history.append({
                    'step': state.global_step,
                    'eval_loss': eval_loss,
                    'cer': cer_result
                })
                
            except Exception as e:
                print(f"âŒ CER è¨ˆç®—å¤±æ•—: {e}")
                # å³ä½¿å¤±æ•—ä¹Ÿè¦è¨˜éŒ„
                self._save_cer_result(state, eval_loss, None, error=str(e))
        else:
            print(f"â­ï¸ eval_loss ({eval_loss:.4f}) >= {self.eval_loss_threshold}ï¼Œè·³é CER è©•ä¼°")
            # è¨˜éŒ„è·³éçš„è©•ä¼°
            self._save_cer_result(state, eval_loss, None, skipped=True)
            
        return control
    
    def _save_cer_result(self, state, eval_loss, cer_result, error=None, skipped=False):
        """ä¿å­˜ CER çµæœåˆ°æ—¥èªŒæ–‡ä»¶"""
        if self.cer_log_file is None:
            return
            
        try:
            # è®€å–ç¾æœ‰çš„æ—¥èªŒ
            with open(self.cer_log_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            # æº–å‚™æ–°çš„è©•ä¼°è¨˜éŒ„
            evaluation_record = {
                "step": state.global_step,
                "epoch": state.epoch,
                "eval_loss": eval_loss,
                "timestamp": datetime.now().isoformat(),
            }
            
            if skipped:
                evaluation_record["status"] = "skipped"
                evaluation_record["reason"] = f"eval_loss ({eval_loss:.4f}) >= threshold ({self.eval_loss_threshold})"
            elif error:
                evaluation_record["status"] = "error"
                evaluation_record["error"] = error
            else:
                evaluation_record["status"] = "completed"
                evaluation_record["cer"] = cer_result
                
            # æ·»åŠ åˆ°è©•ä¼°åˆ—è¡¨
            log_data["evaluations"].append(evaluation_record)
            
            # ä¿å­˜æ›´æ–°çš„æ—¥èªŒ
            with open(self.cer_log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ CER æ—¥èªŒæ™‚å‡ºéŒ¯: {e}")
    
    def _compute_cer(self, model, args):
        """è¨ˆç®— CER"""
        model.eval()
        
        # å‰µå»º DataLoader
        eval_dataloader = DataLoader(
            self.eval_dataset,
            batch_size=8,  # ä½¿ç”¨è¼ƒå°çš„ batch size é¿å…è¨˜æ†¶é«”å•é¡Œ
            collate_fn=self.data_collator,
            shuffle=False,
            num_workers=2
        )
        
        # åˆå§‹åŒ– CER è¨ˆç®—å™¨
        metric = evaluate.load("cer")
        
        print(f"ğŸ”„ é–‹å§‹ CER è©•ä¼°ï¼Œå…± {len(eval_dataloader)} å€‹æ‰¹æ¬¡...")
        
        with torch.no_grad():
            for step, batch in enumerate(tqdm(eval_dataloader, desc="CERè©•ä¼°")):
                with torch.autocast(device_type="cuda"):
                    # ä¿æŒåœ¨ GPU ä¸Šé€²è¡Œé‹ç®—
                    generated_tokens = model.generate(
                        input_features=batch["input_features"].cuda(),
                        decoder_input_ids=batch["labels"][:, :4].cuda(),
                        max_new_tokens=255)
                    labels = batch["labels"]
                    # åœ¨ GPU ä¸Šè™•ç† labels
                    labels = torch.where(labels != -100, labels, self.processor.tokenizer.pad_token_id)
                    
                    # åªåœ¨éœ€è¦ decode æ™‚æ‰ç§»åˆ° CPU
                    generated_tokens = generated_tokens.cpu().numpy()
                    labels = labels.cpu().numpy()
                    
                    # å°‡é æ¸¬å’Œå¯¦éš›çš„ token è½‰æ›ç‚ºæ–‡æœ¬
                    decoded_preds = self.processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
                    decoded_labels = self.processor.tokenizer.batch_decode(labels, skip_special_tokens=True)
                    
                    # åˆªé™¤æ¨™é»ç¬¦è™Ÿ
                    if self.remove_pun:
                        decoded_preds = remove_punctuation(decoded_preds)
                        decoded_labels = remove_punctuation(decoded_labels)
                    
                    # è½‰æ›ç‚ºç°¡é«”ä¸­æ–‡
                    if self.to_simple:
                        decoded_preds = to_simple(decoded_preds)
                        decoded_labels = to_simple(decoded_labels)
                    
                    metric.add_batch(predictions=decoded_preds, references=decoded_labels)
                
                # åªè©•ä¼°å‰å¹¾å€‹æ‰¹æ¬¡ä»¥ç¯€çœæ™‚é–“
                if step >= 20:  # é™åˆ¶è©•ä¼°æ‰¹æ¬¡æ•¸é‡
                    print(f"âš¡ é™åˆ¶è©•ä¼°æ‰¹æ¬¡æ•¸é‡ï¼Œåªè©•ä¼°å‰ {step+1} å€‹æ‰¹æ¬¡")
                    break
        
        # è¨ˆç®—ä¸¦è¿”å› CER
        cer_result = metric.compute()
        model.train()  # åˆ‡å›è¨“ç·´æ¨¡å¼
        return cer_result
