import argparse
import functools
import os
import platform

from peft import (
    LoraConfig,
    get_peft_model,
    AdaLoraConfig,
    PeftModel,
    prepare_model_for_kbit_training,
)
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperForConditionalGeneration,
    WhisperProcessor,
)

from utils.callback import SavePeftModelCallback, CEREvaluationCallback
from utils.data_utils import DataCollatorSpeechSeq2SeqWithPadding
from utils.model_utils import load_from_checkpoint
from utils.reader import CustomDataset
from utils.utils import print_arguments, make_inputs_require_grad, add_arguments

parser = argparse.ArgumentParser(description=__doc__)
add_arg = functools.partial(add_arguments, argparser=parser)
add_arg("train_data", type=str, default="dataset/train.json", help="è®­ç»ƒæ•°æ®é›†çš„è·¯å¾„")
add_arg("test_data", type=str, default="dataset/test.json", help="æµ‹è¯•æ•°æ®é›†çš„è·¯å¾„")
add_arg("base_model", type=str, default="openai/whisper-tiny", help="Whisperçš„åŸºç¡€æ¨¡å‹")
add_arg("output_dir", type=str, default="output/", help="è®­ç»ƒä¿å­˜æ¨¡å‹çš„è·¯å¾„")
add_arg("warmup_steps", type=int, default=50, help="è®­ç»ƒé¢„çƒ­æ­¥æ•°")
add_arg("logging_steps", type=int, default=100, help="æ‰“å°æ—¥å¿—æ­¥æ•°")
add_arg("eval_steps", type=int, default=1000, help="å¤šå°‘æ­¥æ•°è¯„ä¼°ä¸€æ¬¡")
add_arg("save_steps", type=int, default=1000, help="å¤šå°‘æ­¥æ•°ä¿å­˜æ¨¡å‹ä¸€æ¬¡")
add_arg("num_workers", type=int, default=8, help="è¯»å–æ•°æ®çš„çº¿ç¨‹æ•°é‡")
add_arg("learning_rate", type=float, default=1e-3, help="å­¦ä¹ ç‡å¤§å°")
add_arg("min_audio_len", type=float, default=0.5, help="æœ€å°çš„éŸ³é¢‘é•¿åº¦ï¼Œå•ä½ç§’")
add_arg(
    "max_audio_len", type=float, default=30, help="æœ€å¤§çš„éŸ³é¢‘é•¿åº¦ï¼Œå•ä½ç§’ï¼Œä¸èƒ½å¤§äº30ç§’"
)
add_arg("use_adalora", type=bool, default=True, help="æ˜¯å¦ä½¿ç”¨AdaLoraè€Œä¸æ˜¯Lora")
add_arg("fp16", type=bool, default=True, help="æ˜¯å¦ä½¿ç”¨fp16è®­ç»ƒæ¨¡å‹")
add_arg("use_8bit", type=bool, default=False, help="æ˜¯å¦å°†æ¨¡å‹é‡åŒ–ä¸º8ä½")
add_arg("timestamps", type=bool, default=False, help="è®­ç»ƒæ—¶æ˜¯å¦ä½¿ç”¨æ—¶é—´æˆ³æ•°æ®")
add_arg("use_compile", type=bool, default=False, help="æ˜¯å¦ä½¿ç”¨Pytorch2.0çš„ç¼–è¯‘å™¨")
add_arg(
    "local_files_only",
    type=bool,
    default=False,
    help="æ˜¯å¦åªåœ¨æœ¬åœ°åŠ è½½æ¨¡å‹ï¼Œä¸å°è¯•ä¸‹è½½",
)
add_arg("num_train_epochs", type=int, default=3, help="è®­ç»ƒçš„è½®æ•°")
add_arg(
    "language",
    type=str,
    default="Chinese",
    help="è®¾ç½®è¯­è¨€ï¼Œå¯å…¨ç§°ä¹Ÿå¯ç®€å†™ï¼Œå¦‚æœä¸ºNoneåˆ™è®­ç»ƒçš„æ˜¯å¤šè¯­è¨€",
)
add_arg(
    "task",
    type=str,
    default="transcribe",
    choices=["transcribe", "translate"],
    help="æ¨¡å‹çš„ä»»åŠ¡",
)
add_arg("augment_config_path", type=str, default=None, help="æ•°æ®å¢å¼ºé…ç½®æ–‡ä»¶è·¯å¾„")
add_arg("resume_from_checkpoint", type=str, default=None, help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„")
add_arg("per_device_train_batch_size", type=int, default=8, help="è®­ç»ƒçš„batch size")
add_arg("per_device_eval_batch_size", type=int, default=8, help="è¯„ä¼°çš„batch size")
add_arg("gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
add_arg(
    "push_to_hub", type=bool, default=False, help="æ˜¯å¦å°†æ¨¡å‹æƒé‡æ¨åˆ°HuggingFace Hub"
)
add_arg("hub_model_id", type=str, default=None, help="HuggingFace Hubä¸Šçš„æ¨¡å‹ä»“åº“ID")
add_arg("save_total_limit", type=int, default=100, help="åªä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹çš„æ•°é‡")
add_arg(
    "override_training_args",
    type=bool,
    default=True,
    help="æ˜¯å¦è¦†ç›–checkpointä¸­çš„è®­ç»ƒå‚æ•°",
)
args = parser.parse_args()
print_arguments(args)

# å¦‚æœæ˜¯Windowsï¼Œnum_workersè®¾ç½®ä¸º0
if platform.system() == "Windows":
    args.num_workers = 0


def main():
    # è·å–Whisperçš„æ•°æ®å¤„ç†å™¨ï¼Œè¿™ä¸ªåŒ…å«äº†ç‰¹å¾æå–å™¨ã€tokenizer
    processor = WhisperProcessor.from_pretrained(
        args.base_model,
        language=args.language,
        task=args.task,
        no_timestamps=not args.timestamps,
        local_files_only=args.local_files_only,
    )

    # è¯»å–æ•°æ®
    train_dataset = CustomDataset(
        data_list_path=args.train_data,
        processor=processor,
        language=args.language,
        timestamps=args.timestamps,
        min_duration=args.min_audio_len,
        max_duration=args.max_audio_len,
        augment_config_path=args.augment_config_path,
    )
    test_dataset = CustomDataset(
        data_list_path=args.test_data,
        processor=processor,
        language=args.language,
        timestamps=args.timestamps,
        min_duration=args.min_audio_len,
        max_duration=args.max_audio_len,
    )
    print(f"è®­ç»ƒæ•°æ®ï¼š{len(train_dataset)}ï¼Œæµ‹è¯•æ•°æ®ï¼š{len(test_dataset)}")
    # æ•°æ®paddingå™¨
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

    # è·å–Whisperæ¨¡å‹
    device_map = "auto"
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    ddp = world_size != 1
    if ddp:
        device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}

    # è·å–æ¨¡å‹
    model = WhisperForConditionalGeneration.from_pretrained(
        args.base_model,
        load_in_8bit=args.use_8bit,
        device_map=device_map,
        local_files_only=args.local_files_only,
    )
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []

    # èª¿æ•´æ¨¡å‹ä»¥é©æ‡‰æ“´å±•çš„è©å½™è¡¨ï¼ˆå¦‚æœæœ‰æ·»åŠ è‡ªå®šç¾©èªè¨€ tokenï¼‰
    original_vocab_size = len(processor.tokenizer.get_vocab())
    if len(processor.tokenizer.get_vocab()) > model.config.vocab_size:
        print(
            f"ğŸ“ˆ è©å½™è¡¨å·²æ“´å±•ï¼š{model.config.vocab_size} -> {len(processor.tokenizer.get_vocab())}"
        )
        print("ğŸ”§ èª¿æ•´æ¨¡å‹ embedding å±¤å¤§å°...")

        # èª¿æ•´æ¨¡å‹çš„ embedding å±¤
        model.resize_token_embeddings(len(processor.tokenizer.get_vocab()))

        # æ›´æ–°æ¨¡å‹é…ç½®
        model.config.vocab_size = len(processor.tokenizer.get_vocab())

        print(f"âœ… æ¨¡å‹ embedding å±¤å·²èª¿æ•´ç‚º {model.config.vocab_size} tokens")

    # é‡åŒ–æ¨¡å‹
    model = prepare_model_for_kbit_training(model)
    # æ³¨å†Œforwardï¼Œå¦åˆ™å¤šå¡è®­ç»ƒä¼šå¤±è´¥
    model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)

    print("åŠ è½½LoRAæ¨¡å—...")
    if args.resume_from_checkpoint:
        # æ¢å¤è®­ç»ƒæ—¶åŠ è½½Loraå‚æ•°
        print("Loading adapters from checkpoint.")
        model = PeftModel.from_pretrained(
            model, args.resume_from_checkpoint, is_trainable=True
        )
    else:
        print(f"adding LoRA modules...")
        target_modules = ["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"]
        print(target_modules)
        if args.use_adalora:
            total_step = args.num_train_epochs * len(train_dataset)
            config = AdaLoraConfig(
                init_r=12,
                target_r=4,
                beta1=0.85,
                beta2=0.85,
                tinit=200,
                tfinal=1000,
                deltaT=10,
                lora_alpha=32,
                lora_dropout=0.1,
                orth_reg_weight=0.5,
                target_modules=target_modules,
                total_step=total_step,
            )
        else:
            config = LoraConfig(
                r=32,
                lora_alpha=64,
                target_modules=target_modules,
                lora_dropout=0.05,
                bias="none",
            )
        model = get_peft_model(model, config)

    if args.base_model.endswith("/"):
        args.base_model = args.base_model[:-1]
    output_dir = str(os.path.join(args.output_dir, os.path.basename(args.base_model)))
    # å®šä¹‰è®­ç»ƒå‚æ•°
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,  # ä¿å­˜æ£€æŸ¥ç‚¹å’Œæ„å¿—çš„ç›®å½•
        per_device_train_batch_size=args.per_device_train_batch_size,  # è®­ç»ƒbatch_sizeå¤§å°
        per_device_eval_batch_size=args.per_device_eval_batch_size,  # è¯„ä¼°batch_sizeå¤§å°
        gradient_accumulation_steps=args.gradient_accumulation_steps,  # è®­ç»ƒæ¢¯åº¦ç´¯è®¡æ­¥æ•°
        learning_rate=args.learning_rate,  # å­¦ä¹ ç‡å¤§å°
        warmup_steps=args.warmup_steps,  # é¢„çƒ­æ­¥æ•°
        num_train_epochs=args.num_train_epochs,  # å¾®è°ƒè®­ç»ƒè½®æ•°
        save_strategy="steps",  # æŒ‡å®šæŒ‰ç…§æ­¥æ•°ä¿å­˜æ£€æŸ¥ç‚¹
        eval_strategy="steps",  # æŒ‡å®šæŒ‰ç…§æ­¥æ•°è¯„ä¼°æ¨¡å‹
        load_best_model_at_end=True,  # æŒ‡å®šæ˜¯å¦åœ¨ç»“æŸæ—¶åŠ è½½æœ€ä¼˜æ¨¡å‹
        fp16=args.fp16,  # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒ
        report_to=["tensorboard"],  # æŒ‡å®šä½¿ç”¨tensorboardä¿å­˜log
        save_steps=args.save_steps,  # æŒ‡å®šä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°
        eval_steps=args.eval_steps,  # æŒ‡å®šè¯„ä¼°æ¨¡å‹çš„æ­¥æ•°
        torch_compile=args.use_compile,  # ä½¿ç”¨Pytorch2.0çš„ç¼–è¯‘å™¨
        save_total_limit=args.save_total_limit,  # åªä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹çš„æ•°é‡
        optim="adamw_torch",  # æŒ‡å®šä¼˜åŒ–æ–¹æ³•
        ddp_find_unused_parameters=False if ddp else None,  # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        dataloader_num_workers=args.num_workers,  # è®¾ç½®è¯»å–æ•°æ®çš„çº¿ç¨‹æ•°é‡
        logging_steps=args.logging_steps,  # æŒ‡å®šæ‰“å°logçš„æ­¥æ•°
        remove_unused_columns=False,  # åˆ é™¤æ¨¡å‹ä¸éœ€è¦çš„æ•°æ®åˆ—
        label_names=["labels"],  # ä¸æ ‡ç­¾å¯¹åº”çš„è¾“å…¥å­—å…¸ä¸­çš„é”®åˆ—è¡¨
        push_to_hub=args.push_to_hub,  # æ˜¯å¦å°†æ¨¡å‹æƒé‡æ¨åˆ°HuggingFace Hub
    )

    if training_args.local_rank == 0 or training_args.local_rank == -1:
        print("=" * 90)
        model.print_trainable_parameters()
        print("=" * 90)

    # å‰µå»º CER è©•ä¼° callback
    cer_callback = CEREvaluationCallback(
        eval_dataset=test_dataset,
        processor=processor,
        eval_loss_threshold=0.3,  # åªæœ‰ç•¶ eval_loss < 0.3 æ™‚æ‰é€²è¡Œ CER è©•ä¼°
        remove_pun=True,
        to_simple=True,
    )

    # å®šä¹‰è®­ç»ƒå™¨
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        processing_class=processor.feature_extractor,
        callbacks=[SavePeftModelCallback, cer_callback],
    )
    model.config.use_cache = False
    trainer._load_from_checkpoint = load_from_checkpoint

    # å¼€å§‹è®­ç»ƒ
    if args.resume_from_checkpoint and not args.override_training_args:
        # ä½¿ç”¨checkpointä¸­çš„è®­ç»ƒå‚æ•°
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    elif args.resume_from_checkpoint and args.override_training_args:
        # è¦†ç›–checkpointä¸­çš„è®­ç»ƒå‚æ•°ï¼ŒåªåŠ è½½æ¨¡å‹æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€
        print(f"âš ï¸  è¦†ç›–checkpointä¸­çš„è®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨å½“å‰è®¾å®šçš„å‚æ•°")
        print(f"ğŸ“‚ ä» {args.resume_from_checkpoint} åŠ è½½æ¨¡å‹æƒé‡")
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    else:
        # ä»å¤´å¼€å§‹è®­ç»ƒ
        trainer.train()

    # ä¿å­˜æœ€åçš„æ¨¡å‹
    trainer.save_state()
    # é‡æ–°å¯ç”¨ç¼“å­˜ä»¥æ›´å¿«åœ°æ¨æ–­
    model.config.use_cache = True
    if training_args.local_rank == 0 or training_args.local_rank == -1:
        model.save_pretrained(os.path.join(output_dir, "checkpoint-final"))
    # æ˜¯å¦æŠŠæ¨¡å‹å‚æ•°æ–‡ä»¶æ¨é€åˆ°huggingface
    if training_args.push_to_hub:
        hub_model_id = (
            args.hub_model_id if args.hub_model_id is not None else output_dir
        )
        model.push_to_hub(hub_model_id)


if __name__ == "__main__":
    main()
