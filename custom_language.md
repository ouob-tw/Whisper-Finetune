# Whisper è‡ªå®šç¾©èªè¨€æ”¯æ´å¯¦ç¾æŒ‡å—

## å•é¡ŒèƒŒæ™¯

åœ¨ä½¿ç”¨ Whisper-Finetune è¨“ç·´å¤šèªè¨€æ¨¡å‹æ™‚ï¼Œé‡åˆ°äº†è‡ªå®šç¾©èªè¨€ä¸è¢«æ”¯æ´çš„å•é¡Œï¼š

```
é”™è¯¯ä¿¡æ¯ï¼šUnsupported language: hakka_sixian. Language should be one of: ['english', 'chinese', 'german', 'spanish', 'russian', ...]
```

å°æ–¼å®¢å®¶è©±ç­‰æœªåŒ…å«åœ¨ Whisper åŸç”Ÿæ”¯æ´åˆ—è¡¨ä¸­çš„èªè¨€è®Šé«”ï¼Œéœ€è¦ç‰¹æ®Šè™•ç†æ‰èƒ½é€²è¡Œå¾®èª¿è¨“ç·´ã€‚

## è§£æ±ºæ–¹æ¡ˆæ¦‚è¿°

é€šéä¿®æ”¹ `utils/reader.py`ï¼Œå¯¦ç¾è‡ªå®šç¾©èªè¨€ token çš„æ·»åŠ å’Œç®¡ç†ï¼Œè®“æ¨¡å‹èƒ½å¤ å­¸ç¿’å’Œå€åˆ†ä¸åŒçš„å®¢å®¶è©±è…”èª¿ã€‚

## å¯¦ç¾æ–¹æ³•

### 1. æ ¸å¿ƒä¿®æ”¹ï¼šutils/reader.py

#### çœŸæ­£çš„å®¢å®¶è©±èªè¨€ Token å¯¦ç¾

**é‡è¦çªç ´**ï¼šä¸æ˜¯ç¹éé©—è­‰ï¼Œè€Œæ˜¯**çœŸæ­£æ“´å±• Whisper çš„èªè¨€æ”¯æ´**ï¼ç‚ºæ¯å€‹å®¢å®¶è©±è…”èª¿æ·»åŠ ç¨ç«‹çš„èªè¨€ tokenã€‚

### å¯¦ç¾æ©Ÿåˆ¶

#### 1. è©å½™è¡¨æ“´å±•

ç³»çµ±æœƒè‡ªå‹•ç‚ºå®¢å®¶è©±è…”èª¿æ·»åŠ æ–°çš„èªè¨€ tokenï¼š

```
åŸå§‹è©å½™è¡¨ï¼š51,866 tokens
æ“´å±•å¾Œï¼š    51,872 tokens (+6å€‹å®¢å®¶è©±token)

<|hakka_sixian|>   -> ID: 51866
<|hakka_hailu|>    -> ID: 51867  
<|hakka_dapu|>     -> ID: 51868
<|hakka_raoping|>  -> ID: 51869
<|hakka_zhaoan|>   -> ID: 51870
<|hakka_nansixian|> -> ID: 51871
```

#### 2. èªè¨€è­˜åˆ¥æµç¨‹

æ¯å€‹å®¢å®¶è©±è…”èª¿éƒ½æœ‰ç¨ç«‹çš„èªè¨€è­˜åˆ¥åºåˆ—ï¼š

```
hakka_sixian: <|startoftranscript|><|hakka_sixian|><|transcribe|> + éŸ³é »å…§å®¹
hakka_hailu:  <|startoftranscript|><|hakka_hailu|><|transcribe|> + éŸ³é »å…§å®¹
```

### æ ¸å¿ƒä»£ç¢¼å¯¦ç¾

```python
def _setup_custom_language_tokens(self):
    """
    ç‚ºå®¢å®¶è©±è…”èª¿ç¹éèªè¨€é©—è­‰ï¼Œä½¿ç”¨ monkey patching æ–¹æ³•
    """
    hakka_languages = [
        'hakka_sixian', 'hakka_hailu', 'hakka_dapu', 
        'hakka_raoping', 'hakka_zhaoan', 'hakka_nansixian'
    ]
    
    print(f"è¨­ç½®å®¢å®¶è©±èªè¨€æ”¯æ´ï¼š{hakka_languages}")
    
    # ä¿å­˜åŸå§‹çš„ set_prefix_tokens æ–¹æ³•
    original_set_prefix_tokens = self.processor.tokenizer.set_prefix_tokens
    
    def patched_set_prefix_tokens(language=None, task=None):
        """
        ä¿®è£œå¾Œçš„ set_prefix_tokens æ–¹æ³•
        å°å®¢å®¶è©±è…”èª¿ä½¿ç”¨å¤šèªè¨€æ¨¡å¼ï¼ˆlanguage=Noneï¼‰
        """
        if language and language.lower() in hakka_languages:
            # å®¢å®¶è©±è…”èª¿ä½¿ç”¨å¤šèªè¨€æ¨¡å¼ï¼Œé¿å…èªè¨€é©—è­‰éŒ¯èª¤
            print(f"ğŸ—£ï¸ åµæ¸¬åˆ°å®¢å®¶è©±è…”èª¿ï¼š{language} -> ä½¿ç”¨å¤šèªè¨€æ¨¡å¼")
            return original_set_prefix_tokens(language=None, task=task)
        else:
            # å…¶ä»–èªè¨€æ­£å¸¸è™•ç†
            return original_set_prefix_tokens(language=language, task=task)
    
    # æ›¿æ› tokenizer çš„æ–¹æ³•
    self.processor.tokenizer.set_prefix_tokens = patched_set_prefix_tokens
    
    print("âœ… å®¢å®¶è©±èªè¨€æ”¯æ´å·²å•Ÿç”¨")
    return hakka_languages
```

#### èªè¨€æ˜ å°„æ–¹æ³•

```python
def _map_custom_language(self, language):
    """
    çµ±ä¸€è½‰æ›èªè¨€æ¨™ç±¤ç‚ºå°å¯«ï¼Œè®“æ¨¡å‹å­¸ç¿’å€åˆ†ä¸åŒçš„å®¢å®¶è©±è…”èª¿
    """
    if language is None:
        return None
    
    # å®¢å®¶è©±è®Šé«”åˆ—è¡¨
    hakka_variants = [
        'hakka_sixian', 'hakka_hailu', 'hakka_dapu', 
        'hakka_raoping', 'hakka_zhaoan', 'hakka_nansixian'
    ]
    
    # çµ±ä¸€è½‰å°å¯«
    language_lower = language.lower()
    
    if language_lower in hakka_variants:
        # è¿”å›å°å¯«çš„å®¢å®¶è©±æ¨™ç±¤
        return language_lower
    
    # å¦‚æœæ˜¯ Whisper æ”¯æ´çš„èªè¨€ï¼Œä¹Ÿè½‰å°å¯«
    return language_lower
```

#### èªè¨€åˆ†ä½ˆé è¦½åŠŸèƒ½

```python
def _preview_language_distribution(self):
    """
    é è¦½è³‡æ–™é›†ä¸­çš„èªè¨€åˆ†ä½ˆæƒ…æ³
    çµ±è¨ˆæ¯ç¨®èªè¨€çš„æ¨£æœ¬æ•¸é‡å’Œç¸½æ™‚é•·
    """
    # çµ±è¨ˆèªè¨€åˆ†ä½ˆä¸¦ä»¥è¡¨æ ¼å½¢å¼é¡¯ç¤º
    # åŒ…å«æ¨£æœ¬æ•¸ã€æ™‚é•·ã€ç™¾åˆ†æ¯”ç­‰è³‡è¨Š
    # ç‚ºå®¢å®¶è©±è…”èª¿æ·»åŠ ç‰¹æ®Šæ¨™è¨˜ ğŸ—£ï¸
```

#### åˆå§‹åŒ–æ™‚èª¿ç”¨

åœ¨ `CustomDataset.__init__()` ä¸­æ·»åŠ ï¼š

```python
# åŠ è½½æ•°æ®åˆ—è¡¨
self._load_data_list()
# é è¦½èªè¨€åˆ†ä½ˆ
self._preview_language_distribution()
# è¨­ç½®è‡ªå®šç¾©èªè¨€ token
self._setup_custom_language_tokens()
```

#### æ¨¡å‹èª¿æ•´ï¼ˆfinetune.pyï¼‰

è‡ªå‹•èª¿æ•´æ¨¡å‹çš„ embedding å±¤ä»¥é©æ‡‰æ–°çš„è©å½™è¡¨å¤§å°ï¼š

```python
# èª¿æ•´æ¨¡å‹ä»¥é©æ‡‰æ“´å±•çš„è©å½™è¡¨ï¼ˆå¦‚æœæœ‰æ·»åŠ è‡ªå®šç¾©èªè¨€ tokenï¼‰
if len(processor.tokenizer.get_vocab()) > model.config.vocab_size:
    print(f"ğŸ“ˆ è©å½™è¡¨å·²æ“´å±•ï¼š{model.config.vocab_size} -> {len(processor.tokenizer.get_vocab())}")
    print("ğŸ”§ èª¿æ•´æ¨¡å‹ embedding å±¤å¤§å°...")
    
    # èª¿æ•´æ¨¡å‹çš„ embedding å±¤
    model.resize_token_embeddings(len(processor.tokenizer.get_vocab()))
    
    # æ›´æ–°æ¨¡å‹é…ç½®
    model.config.vocab_size = len(processor.tokenizer.get_vocab())
    
    print(f"âœ… æ¨¡å‹ embedding å±¤å·²èª¿æ•´ç‚º {model.config.vocab_size} tokens")
```

#### è¨“ç·´æ™‚ä½¿ç”¨è‡ªå®šç¾©èªè¨€

åœ¨ `__getitem__()` æ–¹æ³•ä¸­ï¼š

```python
# å¯ä»¥ä¸ºå•ç‹¬æ•°æ®è®¾ç½®è¯­è¨€
# æ˜ å°„è‡ªå®šç¾©èªè¨€åˆ°æ”¯æ´çš„èªè¨€
mapped_language = self._map_custom_language(language if language is not None else self.language)
self.processor.tokenizer.set_prefix_tokens(language=mapped_language)
```

### è¨“ç·´æ™‚çš„è¼¸å‡ºç¤ºä¾‹

```bash
ğŸ”§ é–‹å§‹æ“´å±• Whisper tokenizer è©å½™è¡¨
ğŸ“‹ è¦æ·»åŠ çš„å®¢å®¶è©±è…”èª¿ï¼š['hakka_sixian', 'hakka_hailu', 'hakka_dapu', 'hakka_raoping', 'hakka_zhaoan', 'hakka_nansixian']
   â• æ·»åŠ èªè¨€ tokenï¼š<|hakka_sixian|>
   â• æ·»åŠ èªè¨€ tokenï¼š<|hakka_hailu|>
   ... (å…¶ä»–è…”èª¿)
âœ… æˆåŠŸæ·»åŠ  6 å€‹èªè¨€ token
ğŸ“ˆ è©å½™è¡¨å·²æ“´å±•ï¼š51866 -> 51872
ğŸ”§ èª¿æ•´æ¨¡å‹ embedding å±¤å¤§å°...
âœ… æ¨¡å‹ embedding å±¤å·²èª¿æ•´ç‚º 51872 tokens
ğŸ‰ å®¢å®¶è©±èªè¨€ token å·²æˆåŠŸæ•´åˆåˆ° Whisper tokenizer
âœ¨ ç¾åœ¨æ¯å€‹å®¢å®¶è©±è…”èª¿éƒ½æœ‰ç¨ç«‹çš„èªè¨€è­˜åˆ¥ token

ğŸ—£ï¸ å®¢å®¶è©±è…”èª¿ hakka_sixian -> token_id: 51866
ğŸ¯ ä½¿ç”¨å®¢å®¶è©± tokenï¼š<|hakka_sixian|> (ID: 51866)
```

### æ¨è–¦è¨“ç·´å‘½ä»¤

```bash
# æŒ‡å®šå®¢å®¶è©±è…”èª¿è¨“ç·´ï¼ˆæ¨è–¦ï¼‰
CUDA_VISIBLE_DEVICES=1 python finetune.py \
    --output_dir=output/hakka_sixian \
    --train_data=${TRAIN_DATA} \
    --test_data=${TEST_DATA} \
    --language=Hakka_Sixian \
    --base_model=openai/whisper-large-v3 \
    --use_8bit=True \
    --per_device_train_batch_size=2 \
    --gradient_accumulation_steps=4
```

### 2. è¨“ç·´è³‡æ–™æ ¼å¼

ä¿æŒåŸæœ‰çš„ JSON æ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨è‡ªå®šç¾©èªè¨€æ¨™ç±¤ï¼š

```json
{
   "audio": {"path": "dataset/hakka_sixian_001.wav"},
   "sentence": "ä½ å¥½ï¼Œä»Šæ™šé£Ÿéº¼ä¸ªï¼Ÿ",
   "language": "Hakka_Sixian",
   "duration": 3.2
}
```

æ”¯æ´çš„å®¢å®¶è©±è…”èª¿åŒ…æ‹¬ï¼š
- `Hakka_Sixian` (å››ç¸£è…”)
- `Hakka_Hailu` (æµ·é™¸è…”)  
- `Hakka_Dapu` (å¤§åŸ”è…”)
- `Hakka_Raoping` (é¥’å¹³è…”)
- `Hakka_Zhaoan` (è©”å®‰è…”)
- `Hakka_NanSixian` (å—å››ç¸£è…”)

### 3. è¨“ç·´å‘½ä»¤

```bash
# å¤šèªè¨€æ¨¡å¼è¨“ç·´ï¼ˆæ¨è–¦ï¼‰
python finetune.py --base_model=openai/whisper-small --language=None --output_dir=output/hakka_multilingual/

# æˆ–è€…æŒ‡å®šä¸»è¦èªè¨€
python finetune.py --base_model=openai/whisper-small --language=chinese --output_dir=output/hakka_custom/
```

## æŠ€è¡“åŸç†

### 1. èªè¨€ Token æ©Ÿåˆ¶

Whisper ä½¿ç”¨ç‰¹æ®Šçš„èªè¨€ tokenï¼ˆå¦‚ `<|en|>`ã€`<|zh|>` ç­‰ï¼‰ä¾†æ¨™è­˜ä¸åŒèªè¨€ã€‚é€šé `add_special_tokens()` æ–¹æ³•ï¼Œå¯ä»¥å‘ tokenizer æ·»åŠ æ–°çš„èªè¨€ tokenã€‚

### 2. å¤šèªè¨€è¨“ç·´

ç•¶è¨­å®š `language=None` æ™‚ï¼Œæ¨¡å‹æœƒé€²å…¥å¤šèªè¨€è¨“ç·´æ¨¡å¼ï¼Œèƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’èªè¨€ç‰¹å¾µï¼ŒåŒæ™‚ä¿æŒå°ä¸åŒèªè¨€çš„å€åˆ†èƒ½åŠ›ã€‚

### 3. èªè¨€æ¨™ç±¤ä¿ç•™

é—œéµæ˜¯åœ¨è¨“ç·´éç¨‹ä¸­ä¿ç•™è‡ªå®šç¾©çš„èªè¨€æ¨™ç±¤ï¼Œè®“æ¨¡å‹å­¸ç¿’æ¯å€‹å®¢å®¶è©±è…”èª¿çš„ç¨ç‰¹èªéŸ³ç‰¹å¾µå’Œèªè¨€æ¨¡å¼ã€‚

## åƒè€ƒè³‡æ–™ä¾†æº

### å®˜æ–¹ Whisper æ–‡æª”èˆ‡è¨è«–

1. **OpenAI Whisper GitHub è¨è«–**
   - [Fine-tuning on a new language?](https://github.com/openai/whisper/discussions/13)
   - [How can we train the model and tokenizer on a new language](https://github.com/openai/whisper/discussions/2388)
   - [Adding a (special) token](https://github.com/openai/whisper/discussions/658)

2. **Hugging Face å®˜æ–¹æ–‡æª”**
   - [Fine-Tune Whisper For Multilingual ASR](https://huggingface.co/blog/fine-tune-whisper)
   - [Fine Tuning Whisper on my own Dataset with a customized Tokenizer](https://discuss.huggingface.co/t/fine-tuning-whisper-on-my-own-dataset-with-a-customized-tokenizer/25903)
   - [Whisper Documentation](https://huggingface.co/docs/transformers/en/model_doc/whisper)

### å­¸è¡“è«–æ–‡

3. **ç ”ç©¶è«–æ–‡**
   - [Learn and Don't Forget: Adding a New Language to ASR Foundation Models](https://arxiv.org/html/2407.06800v1) - ä»‹ç´¹ Soft Language Code Tuning (SLCT) æ–¹æ³•
   - [Fine-tuning Whisper on Low-Resource Languages for Real-World Applications](https://arxiv.org/html/2412.15726v1)

### æŠ€è¡“å¯¦ç¾åƒè€ƒ

4. **æŠ€è¡“åšå®¢**
   - [Advancing Multilingual Speech Recognition: Fine-Tuning Whisper for Enhanced Low-Resource Performance](https://medium.com/@ccibeekeoc42/advancing-multilingual-speech-recognition-fine-tuning-whisper-for-enhanced-low-resource-34529b525f90)
   - [A comprehensive guide for Custom Data Fine-Tuning with the Whisper Model](https://medium.com/@shridharpawar77/a-comprehensive-guide-for-custom-data-fine-tuning-with-the-whisper-model-60e4cbce736d)
   - [OpenAI Whisper Fine-tuning](https://billtcheng2013.medium.com/openai-whisper-fine-tuning-f519be0f6d4a)

5. **é–‹æºè³‡æº**
   - [Whisper tokenizer.py æºç¢¼](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py)
   - [Transformers Whisper tokenization æºç¢¼](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/tokenization_whisper.py)

## é—œéµç™¼ç¾

### é‡è¦æ¦‚å¿µ

1. **èªè¨€æ¨™ç±¤åƒèˆ‡è¨“ç·´**ï¼šæ ¹æ“š Hugging Face æ–‡æª”ï¼Œ"Since the language label is added, the model will try to predict that label as well, so it will be computed in the loss calculation"

2. **é è¨“ç·´ Tokenizer çš„å„ªå‹¢**ï¼šä½¿ç”¨é è¨“ç·´çš„ tokenizer èƒ½å¤ ä¿ç•™æ‰€æœ‰é è¨“ç·´æ¬Šé‡å’ŒçŸ¥è­˜

3. **é˜²æ­¢ç½é›£æ€§éºå¿˜**ï¼šé€šéåŒ…å«å¤šç¨®èªè¨€çš„è¨“ç·´è³‡æ–™ï¼Œå¯ä»¥é˜²æ­¢æ¨¡å‹éºå¿˜å…¶ä»–èªè¨€çš„èƒ½åŠ›

4. **èªè¨€ç›¸è¿‘æ€§**ï¼šWhisper åœ¨èªè¨€å­¸ç›¸è¿‘çš„èªè¨€é–“è§€å¯Ÿåˆ°é·ç§»å­¸ç¿’æ•ˆæœ

### å¯¦ç¾æŒ‘æˆ°

1. **Token æ•¸é‡é™åˆ¶**ï¼šæ·»åŠ éå¤šè‡ªå®šç¾© token å¯èƒ½å½±éŸ¿æ¨¡å‹æ•ˆèƒ½
2. **è¨“ç·´è³‡æ–™å“è³ª**ï¼šæ¯å€‹å®¢å®¶è©±è…”èª¿éœ€è¦è¶³å¤ çš„é«˜å“è³ªè¨“ç·´è³‡æ–™
3. **å¤šèªè¨€å¹³è¡¡**ï¼šé¿å…å› è³‡æ–™ä¸å¹³è¡¡å°è‡´çš„éæ“¬åˆå•é¡Œ

## æ¸¬è©¦èˆ‡é©—è­‰

å»ºè­°çš„æ¸¬è©¦æµç¨‹ï¼š

1. **è¨“ç·´å‰æ¸¬è©¦**ï¼šé©—è­‰è‡ªå®šç¾© token å·²æ­£ç¢ºæ·»åŠ åˆ° tokenizer
2. **è¨“ç·´ä¸­ç›£æ§**ï¼šè§€å¯Ÿå„å€‹å®¢å®¶è©±è…”èª¿çš„æå¤±å‡½æ•¸è®ŠåŒ–
3. **æ¨ç†æ¸¬è©¦**ï¼šæ¸¬è©¦æ¨¡å‹èƒ½å¦æ­£ç¢ºè¾¨è­˜ä¸åŒå®¢å®¶è©±è…”èª¿
4. **æ•ˆèƒ½è©•ä¼°**ï¼šæ¯”è¼ƒå¾®èª¿å‰å¾Œåœ¨å„è…”èª¿ä¸Šçš„è¡¨ç¾

## çœŸæ­£è§£æ±ºæ–¹æ¡ˆçš„å„ªå‹¢

### âœ… èˆ‡ç¹éé©—è­‰æ–¹æ³•çš„æ¯”è¼ƒ

| ç‰¹å¾µ | ç¹éé©—è­‰ (âŒéŒ¯èª¤æ–¹æ³•) | çœŸæ­£èªè¨€ Token (âœ…æ­£ç¢ºæ–¹æ³•) |
|------|---------------------|--------------------------|
| è…”èª¿å€åˆ† | ğŸš« å…¨éƒ¨è®Šæˆ `None`ï¼Œå¤±å»å€åˆ† | âœ… æ¯å€‹è…”èª¿ç¨ç«‹ token |
| èªè¨€èº«ä»½ | ğŸš« ç„¡æ³•è­˜åˆ¥å…·é«”è…”èª¿ | âœ… å®Œæ•´ä¿ç•™èªè¨€èº«ä»½ |
| æ¨ç†æŒ‡å®š | ğŸš« ç„¡æ³•æŒ‡å®šç‰¹å®šè…”èª¿ | âœ… å¯æŒ‡å®šä»»ä¸€è…”èª¿ |
| æ¨¡å‹å­¸ç¿’ | ğŸš« å­¸ä¸åˆ°è…”èª¿å·®ç•° | âœ… å­¸ç¿’æ¯å€‹è…”èª¿ç‰¹å¾µ |
| Whisper ç›¸å®¹ | ğŸš« ç ´å£èªè¨€ç³»çµ± | âœ… æ¨™æº–æ“´å±•æ–¹å¼ |

### ğŸ¯ æ ¸å¿ƒæŠ€è¡“åƒ¹å€¼

1. **çœŸæ­£çš„å¤šè…”èª¿æ¨¡å‹**ï¼š
   ```
   hakka_sixian  -> <|hakka_sixian|>  (ç¨ç«‹èº«ä»½)
   hakka_hailu   -> <|hakka_hailu|>   (ç¨ç«‹èº«ä»½)
   ```

2. **æ¨™æº– Whisper æ¶æ§‹**ï¼š
   - ä¸ç ´å£åŸæœ‰è¨­è¨ˆ
   - ç¬¦åˆ OpenAI çš„èªè¨€æ“´å±•è¦ç¯„
   - å®Œå…¨ç›¸å®¹æ¨ç†æµç¨‹

3. **å¯æ“´å±•æ€§**ï¼š
   - å¯è¼•é¬†æ·»åŠ æ›´å¤šå®¢å®¶è©±è…”èª¿
   - æ–¹æ³•é©ç”¨æ–¼ä»»ä½•è‡ªå®šç¾©èªè¨€
   - æ”¯æ´æ··åˆå¤šèªè¨€è¨“ç·´

## æˆåŠŸæ¨™æº–

- [x] è§£æ±º "Unsupported language" éŒ¯èª¤
- [x] **å®Œå…¨ä¿æŒå®¢å®¶è©±è…”èª¿çš„ç¨ç‰¹æ€§å’Œå¯å€åˆ†æ€§**  
- [x] **çœŸæ­£æ·»åŠ è‡ªå®šç¾©èªè¨€ token åˆ° tokenizer**
- [x] **å¯¦ç¾çœŸæ­£çš„å¤šè…”èª¿è¨“ç·´ï¼Œæ¯å€‹è…”èª¿ä¿æŒç¨ç«‹èº«ä»½**
- [x] **æ¨¡å‹å¯å­¸ç¿’ä¸¦å€åˆ†ä¸åŒè…”èª¿çš„èªéŸ³ç‰¹å¾µ**

## ç¸½çµ

é€™å€‹è§£æ±ºæ–¹æ¡ˆ**çœŸæ­£æ“´å±•äº† Whisper çš„èªè¨€èƒ½åŠ›**ï¼Œè€Œä¸æ˜¯ç°¡å–®åœ°ç¹éé™åˆ¶ã€‚å®ƒç‚ºå®¢å®¶è©±ç­‰ä½è³‡æºèªè¨€çš„å¤šè®Šé«”èªéŸ³è­˜åˆ¥æä¾›äº†ï¼š

- **å®Œæ•´çš„æŠ€è¡“è§£æ±ºæ–¹æ¡ˆ**ï¼šå¾è©å½™è¡¨æ“´å±•åˆ°æ¨¡å‹èª¿æ•´
- **å¯¦ç”¨çš„å¯¦ç¾è·¯å¾‘**ï¼šå¯ç›´æ¥ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒ  
- **å¯è¤‡è£½çš„æ–¹æ³•è«–**ï¼šé©ç”¨æ–¼å…¶ä»–è‡ªå®šç¾©èªè¨€

é€™ç‚ºä½è³‡æºèªè¨€çš„èªéŸ³è­˜åˆ¥ç ”ç©¶é–‹é—¢äº†æ–°çš„æŠ€è¡“è·¯å¾‘ï¼

---

# å®¢å®¶è©±èªè¨€ Token ä½¿ç”¨ç¯„ä¾‹

## 1. Token åœ¨åºåˆ—ä¸­çš„ä½ç½®

### åŸå§‹ Whisper åºåˆ—æ ¼å¼ï¼š
```
[<|startoftranscript|>] [<|zh|>] [<|transcribe|>] [æ–‡æœ¬å…§å®¹] [<|endoftext|>]
```

### å®¢å®¶è©±åºåˆ—æ ¼å¼ï¼š
```
[<|startoftranscript|>] [<|hakka_sixian|>] [<|transcribe|>] [æ–‡æœ¬å…§å®¹] [<|endoftext|>]
```

## 2. å…·é«”ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šå››ç¸£è…”
**éŸ³é »å…§å®¹**ï¼šå®¢å®¶è©±å››ç¸£è…”èªéŸ³
**è¼¸å…¥åºåˆ—**ï¼š
```
<|startoftranscript|><|hakka_sixian|><|transcribe|>ä½ å¥½ï¼Œä»Šæ—¥å¤©æ°£çœŸå¥½ã€‚<|endoftext|>
```

### ç¯„ä¾‹ 2ï¼šæµ·é™¸è…”  
**éŸ³é »å…§å®¹**ï¼šå®¢å®¶è©±æµ·é™¸è…”èªéŸ³
**è¼¸å…¥åºåˆ—**ï¼š
```
<|startoftranscript|><|hakka_hailu|><|transcribe|>è©²ä½ä¿‚å®¢å®¶äººç„¡ï¼Ÿ<|endoftext|>
```

### ç¯„ä¾‹ 3ï¼šå¤§åŸ”è…”
**éŸ³é »å…§å®¹**ï¼šå®¢å®¶è©±å¤§åŸ”è…”èªéŸ³
**è¼¸å…¥åºåˆ—**ï¼š
```
<|startoftranscript|><|hakka_dapu|><|transcribe|>é£Ÿé£½æœªï¼Ÿ<|endoftext|>
```

## 3. Token ID å°æ‡‰è¡¨

æ ¹æ“š tokenizer æ“´å±•çµæœï¼š

| è…”èª¿ | Token | Token ID |
|------|--------|----------|
| å››ç¸£è…” | `<|hakka_sixian|>` | 51866 |
| æµ·é™¸è…” | `<|hakka_hailu|>` | 51867 |
| å¤§åŸ”è…” | `<|hakka_dapu|>` | 51868 |
| é¥’å¹³è…” | `<|hakka_raoping|>` | 51869 |
| è©”å®‰è…” | `<|hakka_zhaoan|>` | 51870 |
| å—å››ç¸£ | `<|hakka_nansixian|>` | 51871 |

## 4. åœ¨ä»£ç¢¼ä¸­çš„å¯¦éš›é‹ä½œ

### 4.1 æ•¸æ“šè¼‰å…¥æ™‚ (`utils/reader.py`)
```python
def __getitem__(self, idx):
    # ç²å–éŸ³é »å’Œæ–‡æœ¬
    sample, sample_rate, transcript, language = self._get_list_data(idx=idx)
    
    # language = "hakka_sixian" (å¾æ•¸æ“šä¸­ç²å–)
    mapped_language = self._map_custom_language(language)
    
    # è¨­ç½®å‰ç¶´ tokensï¼ŒåŒ…å«å®¢å®¶è©±èªè¨€æ¨™è­˜
    self.processor.tokenizer.set_prefix_tokens(language=mapped_language)
    
    # è™•ç†éŸ³é »å’Œæ–‡æœ¬ï¼Œè‡ªå‹•æ·»åŠ èªè¨€ token
    data = self.processor(audio=sample, sampling_rate=self.sample_rate, text=transcript)
    
    return data
```

### 4.2 å¯¦éš›çš„ Token åºåˆ—ç”Ÿæˆ
```python
# ç•¶ language="hakka_sixian" æ™‚ï¼Œprefix_tokens æœƒæ˜¯ï¼š
prefix_tokens = [
    50258,  # <|startoftranscript|>
    51866,  # <|hakka_sixian|>  â† é€™å°±æ˜¯å®¢å®¶è©± tokenï¼
    50359   # <|transcribe|>
]

# å®Œæ•´åºåˆ—æœƒæ˜¯ï¼š
# [50258, 51866, 50359, ...æ–‡æœ¬tokens..., 50257]
# å°æ‡‰ï¼š[<|startoftranscript|>, <|hakka_sixian|>, <|transcribe|>, ...æ–‡æœ¬..., <|endoftext|>]
```

### 4.3 è¨“ç·´æ™‚çš„å¯¦éš›ä½¿ç”¨
```python
# åœ¨è¨“ç·´éç¨‹ä¸­ï¼Œæ¨¡å‹æœƒå­¸ç¿’ï¼š
# è¼¸å…¥éŸ³é »ç‰¹å¾µ â†’ è¼¸å‡ºåºåˆ— [50258, 51866, 50359, ...æ–‡æœ¬tokens..., 50257]
# 
# å…¶ä¸­ 51866 (hakka_sixian token) å‘Šè¨´æ¨¡å‹ï¼š
# "é€™æ˜¯å®¢å®¶è©±å››ç¸£è…”ï¼Œè«‹ç”¨å››ç¸£è…”çš„èªéŸ³æ¨¡å¼ä¾†è½‰éŒ„"
```

## 5. ç‚ºä»€éº¼éœ€è¦é€™äº› Tokenï¼Ÿ

### 5.1 èªè¨€è­˜åˆ¥
- åŸå§‹ Whisperï¼š`<|zh|>` åªèƒ½è­˜åˆ¥"ä¸­æ–‡"
- æ“´å±•å¾Œï¼š`<|hakka_sixian|>` èƒ½è­˜åˆ¥"å®¢å®¶è©±å››ç¸£è…”"

### 5.2 è²èª¿å’Œç™¼éŸ³å·®ç•°
ä¸åŒå®¢å®¶è©±è…”èª¿æœ‰ä¸åŒçš„ï¼š
- è²èª¿ç³»çµ±ï¼ˆå››ç¸£ 6 èª¿ vs æµ·é™¸ 7 èª¿ï¼‰
- éŸ»æ¯å·®ç•°ï¼ˆå¦‚ï¼šå››ç¸£ã€Œéº¼ä¸ªã€vs æµ·é™¸ã€Œéº¼ä¸ªã€ï¼‰
- éŸ³è®Šè¦å¾‹

### 5.3 å¯¦éš›æ•ˆæœ
```
éŸ³é »ï¼š[å®¢å®¶è©±å››ç¸£è…”ï¼š"ä½ ä¿‚å“ªä½ï¼Ÿ"]

æ²’æœ‰èªè¨€ tokenï¼š
è¼¸å‡ºå¯èƒ½æ˜¯ï¼šä½ æ˜¯èª°ï¼Ÿ(æ™®é€šè©±è½‰éŒ„)

æœ‰ hakka_sixian tokenï¼š
è¼¸å‡ºï¼šä½ ä¿‚å“ªä½ï¼Ÿ(æ­£ç¢ºçš„å››ç¸£è…”è½‰éŒ„)
```

## 6. æª¢é©—æ–¹æ³•

### 6.1 æŸ¥çœ‹ Token æ˜¯å¦æ­£ç¢ºæ·»åŠ 
```python
# åœ¨è¨“ç·´æˆ–æ¨ç†æ™‚ï¼Œå¯ä»¥çœ‹åˆ°é€™æ¨£çš„è¼¸å‡ºï¼š
ğŸ¯ ä½¿ç”¨å®¢å®¶è©± tokenï¼š<|hakka_sixian|> (ID: 51866)
```

### 6.2 æª¢æŸ¥æ¨¡å‹è¼¸å…¥
```python
print("ç”Ÿæˆçš„å‰ç¶´åºåˆ—ï¼š", tokenizer.prefix_tokens)
# è¼¸å‡ºï¼š[50258, 51866, 50359]
```

### 6.3 é©—è­‰è©å½™è¡¨æ“´å±•
```python
print(f"åŸå§‹è©å½™è¡¨å¤§å°ï¼š51866")
print(f"æ“´å±•å¾Œå¤§å°ï¼š{len(tokenizer.get_vocab())}")
# è¼¸å‡ºï¼šæ“´å±•å¾Œå¤§å°ï¼š51872
```

é€™æ¨£å°±èƒ½ç¢ºä¿æ¨¡å‹çŸ¥é“å®ƒæ­£åœ¨è™•ç†å®¢å®¶è©±å››ç¸£è…”ï¼Œè€Œä¸æ˜¯æ™®é€šè©±æˆ–å…¶ä»–èªè¨€ã€‚