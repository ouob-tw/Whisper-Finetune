# Whisper è‡ªå®šç¾©èªè¨€æ”¯æ´å¯¦ç¾æŒ‡å—

## å•é¡ŒèƒŒæ™¯

åœ¨ä½¿ç”¨ Whisper-Finetune è¨“ç·´å¤šèªè¨€æ¨¡å‹æ™‚ï¼Œé‡åˆ°äº†è‡ªå®šç¾©èªè¨€ä¸è¢«æ”¯æ´çš„å•é¡Œï¼š

```
é”™è¯¯ä¿¡æ¯ï¼šUnsupported language: hakka_sixian. Language should be one of: ['english', 'chinese', 'german', 'spanish', 'russian', ...]
```

å°æ–¼å®¢å®¶è©±ç­‰æœªåŒ…å«åœ¨ Whisper åŸç”Ÿæ”¯æ´åˆ—è¡¨ä¸­çš„èªè¨€è®Šé«”ï¼Œéœ€è¦ç‰¹æ®Šè™•ç†æ‰èƒ½é€²è¡Œå¾®èª¿è¨“ç·´ã€‚

## è§£æ±ºæ–¹æ¡ˆæ¦‚è¿°

é€šéä¿®æ”¹ `utils/reader.py`ï¼Œå¯¦ç¾è‡ªå®šç¾©èªè¨€ token çš„æ·»åŠ å’Œç®¡ç†ï¼Œè®“æ¨¡å‹èƒ½å¤ å­¸ç¿’å’Œå€åˆ†ä¸åŒçš„å®¢å®¶è©±è…”èª¿ã€‚

## å¯¦ç¾æ–¹æ³•

### 1. æ ¸å¿ƒä¿®æ”¹ï¼šutils/reader.py

#### æ·»åŠ è‡ªå®šç¾©èªè¨€ token è¨­ç½®æ–¹æ³•

```python
def _setup_custom_language_tokens(self):
    """
    ç‚ºå®¢å®¶è©±è…”èª¿æ·»åŠ è‡ªå®šç¾©èªè¨€ token åˆ° tokenizer
    é€™æ˜¯åˆå§‹åŒ–æ™‚èª¿ç”¨çš„æ–¹æ³•
    """
    hakka_languages = [
        'Hakka_Sixian', 'Hakka_Hailu', 'Hakka_Dapu', 
        'Hakka_Raoping', 'Hakka_Zhaoan', 'Hakka_NanSixian'
    ]
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ–°çš„èªè¨€ token
    existing_tokens = self.processor.tokenizer.get_vocab()
    new_tokens = []
    
    for lang in hakka_languages:
        token = f"<|{lang.lower()}|>"
        if token not in existing_tokens:
            new_tokens.append(token)
    
    if new_tokens:
        # æ·»åŠ æ–°çš„ç‰¹æ®Š token
        self.processor.tokenizer.add_special_tokens({"additional_special_tokens": new_tokens})
        print(f"Added custom language tokens: {new_tokens}")
    
    return new_tokens
```

#### èªè¨€æ˜ å°„æ–¹æ³•

```python
def _map_custom_language(self, language):
    """
    çµ±ä¸€è½‰æ›èªè¨€æ¨™ç±¤ç‚ºå°å¯«ï¼Œè®“æ¨¡å‹å­¸ç¿’å€åˆ†ä¸åŒçš„å®¢å®¶è©±è…”èª¿
    """
    if language is None:
        return None
    
    # å®¢å®¶è©±è®Šé«”åˆ—è¡¨
    hakka_variants = [
        'hakka_sixian', 'hakka_hailu', 'hakka_dapu', 
        'hakka_raoping', 'hakka_zhaoan', 'hakka_nansixian'
    ]
    
    # çµ±ä¸€è½‰å°å¯«
    language_lower = language.lower()
    
    if language_lower in hakka_variants:
        # è¿”å›å°å¯«çš„å®¢å®¶è©±æ¨™ç±¤
        return language_lower
    
    # å¦‚æœæ˜¯ Whisper æ”¯æ´çš„èªè¨€ï¼Œä¹Ÿè½‰å°å¯«
    return language_lower
```

#### èªè¨€åˆ†ä½ˆé è¦½åŠŸèƒ½

```python
def _preview_language_distribution(self):
    """
    é è¦½è³‡æ–™é›†ä¸­çš„èªè¨€åˆ†ä½ˆæƒ…æ³
    çµ±è¨ˆæ¯ç¨®èªè¨€çš„æ¨£æœ¬æ•¸é‡å’Œç¸½æ™‚é•·
    """
    # çµ±è¨ˆèªè¨€åˆ†ä½ˆä¸¦ä»¥è¡¨æ ¼å½¢å¼é¡¯ç¤º
    # åŒ…å«æ¨£æœ¬æ•¸ã€æ™‚é•·ã€ç™¾åˆ†æ¯”ç­‰è³‡è¨Š
    # ç‚ºå®¢å®¶è©±è…”èª¿æ·»åŠ ç‰¹æ®Šæ¨™è¨˜ ğŸ—£ï¸
```

#### åˆå§‹åŒ–æ™‚èª¿ç”¨

åœ¨ `CustomDataset.__init__()` ä¸­æ·»åŠ ï¼š

```python
# åŠ è½½æ•°æ®åˆ—è¡¨
self._load_data_list()
# é è¦½èªè¨€åˆ†ä½ˆ
self._preview_language_distribution()
# è¨­ç½®è‡ªå®šç¾©èªè¨€ token
self._setup_custom_language_tokens()
```

#### è¨“ç·´æ™‚ä½¿ç”¨è‡ªå®šç¾©èªè¨€

åœ¨ `__getitem__()` æ–¹æ³•ä¸­ï¼š

```python
# å¯ä»¥ä¸ºå•ç‹¬æ•°æ®è®¾ç½®è¯­è¨€
# æ˜ å°„è‡ªå®šç¾©èªè¨€åˆ°æ”¯æ´çš„èªè¨€
mapped_language = self._map_custom_language(language if language is not None else self.language)
self.processor.tokenizer.set_prefix_tokens(language=mapped_language)
```

### 2. è¨“ç·´è³‡æ–™æ ¼å¼

ä¿æŒåŸæœ‰çš„ JSON æ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨è‡ªå®šç¾©èªè¨€æ¨™ç±¤ï¼š

```json
{
   "audio": {"path": "dataset/hakka_sixian_001.wav"},
   "sentence": "ä½ å¥½ï¼Œä»Šæ™šé£Ÿéº¼ä¸ªï¼Ÿ",
   "language": "Hakka_Sixian",
   "duration": 3.2
}
```

æ”¯æ´çš„å®¢å®¶è©±è…”èª¿åŒ…æ‹¬ï¼š
- `Hakka_Sixian` (å››ç¸£è…”)
- `Hakka_Hailu` (æµ·é™¸è…”)  
- `Hakka_Dapu` (å¤§åŸ”è…”)
- `Hakka_Raoping` (é¥’å¹³è…”)
- `Hakka_Zhaoan` (è©”å®‰è…”)
- `Hakka_NanSixian` (å—å››ç¸£è…”)

### 3. è¨“ç·´å‘½ä»¤

```bash
# å¤šèªè¨€æ¨¡å¼è¨“ç·´ï¼ˆæ¨è–¦ï¼‰
python finetune.py --base_model=openai/whisper-small --language=None --output_dir=output/hakka_multilingual/

# æˆ–è€…æŒ‡å®šä¸»è¦èªè¨€
python finetune.py --base_model=openai/whisper-small --language=chinese --output_dir=output/hakka_custom/
```

## æŠ€è¡“åŸç†

### 1. èªè¨€ Token æ©Ÿåˆ¶

Whisper ä½¿ç”¨ç‰¹æ®Šçš„èªè¨€ tokenï¼ˆå¦‚ `<|en|>`ã€`<|zh|>` ç­‰ï¼‰ä¾†æ¨™è­˜ä¸åŒèªè¨€ã€‚é€šé `add_special_tokens()` æ–¹æ³•ï¼Œå¯ä»¥å‘ tokenizer æ·»åŠ æ–°çš„èªè¨€ tokenã€‚

### 2. å¤šèªè¨€è¨“ç·´

ç•¶è¨­å®š `language=None` æ™‚ï¼Œæ¨¡å‹æœƒé€²å…¥å¤šèªè¨€è¨“ç·´æ¨¡å¼ï¼Œèƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’èªè¨€ç‰¹å¾µï¼ŒåŒæ™‚ä¿æŒå°ä¸åŒèªè¨€çš„å€åˆ†èƒ½åŠ›ã€‚

### 3. èªè¨€æ¨™ç±¤ä¿ç•™

é—œéµæ˜¯åœ¨è¨“ç·´éç¨‹ä¸­ä¿ç•™è‡ªå®šç¾©çš„èªè¨€æ¨™ç±¤ï¼Œè®“æ¨¡å‹å­¸ç¿’æ¯å€‹å®¢å®¶è©±è…”èª¿çš„ç¨ç‰¹èªéŸ³ç‰¹å¾µå’Œèªè¨€æ¨¡å¼ã€‚

## åƒè€ƒè³‡æ–™ä¾†æº

### å®˜æ–¹ Whisper æ–‡æª”èˆ‡è¨è«–

1. **OpenAI Whisper GitHub è¨è«–**
   - [Fine-tuning on a new language?](https://github.com/openai/whisper/discussions/13)
   - [How can we train the model and tokenizer on a new language](https://github.com/openai/whisper/discussions/2388)
   - [Adding a (special) token](https://github.com/openai/whisper/discussions/658)

2. **Hugging Face å®˜æ–¹æ–‡æª”**
   - [Fine-Tune Whisper For Multilingual ASR](https://huggingface.co/blog/fine-tune-whisper)
   - [Fine Tuning Whisper on my own Dataset with a customized Tokenizer](https://discuss.huggingface.co/t/fine-tuning-whisper-on-my-own-dataset-with-a-customized-tokenizer/25903)
   - [Whisper Documentation](https://huggingface.co/docs/transformers/en/model_doc/whisper)

### å­¸è¡“è«–æ–‡

3. **ç ”ç©¶è«–æ–‡**
   - [Learn and Don't Forget: Adding a New Language to ASR Foundation Models](https://arxiv.org/html/2407.06800v1) - ä»‹ç´¹ Soft Language Code Tuning (SLCT) æ–¹æ³•
   - [Fine-tuning Whisper on Low-Resource Languages for Real-World Applications](https://arxiv.org/html/2412.15726v1)

### æŠ€è¡“å¯¦ç¾åƒè€ƒ

4. **æŠ€è¡“åšå®¢**
   - [Advancing Multilingual Speech Recognition: Fine-Tuning Whisper for Enhanced Low-Resource Performance](https://medium.com/@ccibeekeoc42/advancing-multilingual-speech-recognition-fine-tuning-whisper-for-enhanced-low-resource-34529b525f90)
   - [A comprehensive guide for Custom Data Fine-Tuning with the Whisper Model](https://medium.com/@shridharpawar77/a-comprehensive-guide-for-custom-data-fine-tuning-with-the-whisper-model-60e4cbce736d)
   - [OpenAI Whisper Fine-tuning](https://billtcheng2013.medium.com/openai-whisper-fine-tuning-f519be0f6d4a)

5. **é–‹æºè³‡æº**
   - [Whisper tokenizer.py æºç¢¼](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py)
   - [Transformers Whisper tokenization æºç¢¼](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/tokenization_whisper.py)

## é—œéµç™¼ç¾

### é‡è¦æ¦‚å¿µ

1. **èªè¨€æ¨™ç±¤åƒèˆ‡è¨“ç·´**ï¼šæ ¹æ“š Hugging Face æ–‡æª”ï¼Œ"Since the language label is added, the model will try to predict that label as well, so it will be computed in the loss calculation"

2. **é è¨“ç·´ Tokenizer çš„å„ªå‹¢**ï¼šä½¿ç”¨é è¨“ç·´çš„ tokenizer èƒ½å¤ ä¿ç•™æ‰€æœ‰é è¨“ç·´æ¬Šé‡å’ŒçŸ¥è­˜

3. **é˜²æ­¢ç½é›£æ€§éºå¿˜**ï¼šé€šéåŒ…å«å¤šç¨®èªè¨€çš„è¨“ç·´è³‡æ–™ï¼Œå¯ä»¥é˜²æ­¢æ¨¡å‹éºå¿˜å…¶ä»–èªè¨€çš„èƒ½åŠ›

4. **èªè¨€ç›¸è¿‘æ€§**ï¼šWhisper åœ¨èªè¨€å­¸ç›¸è¿‘çš„èªè¨€é–“è§€å¯Ÿåˆ°é·ç§»å­¸ç¿’æ•ˆæœ

### å¯¦ç¾æŒ‘æˆ°

1. **Token æ•¸é‡é™åˆ¶**ï¼šæ·»åŠ éå¤šè‡ªå®šç¾© token å¯èƒ½å½±éŸ¿æ¨¡å‹æ•ˆèƒ½
2. **è¨“ç·´è³‡æ–™å“è³ª**ï¼šæ¯å€‹å®¢å®¶è©±è…”èª¿éœ€è¦è¶³å¤ çš„é«˜å“è³ªè¨“ç·´è³‡æ–™
3. **å¤šèªè¨€å¹³è¡¡**ï¼šé¿å…å› è³‡æ–™ä¸å¹³è¡¡å°è‡´çš„éæ“¬åˆå•é¡Œ

## æ¸¬è©¦èˆ‡é©—è­‰

å»ºè­°çš„æ¸¬è©¦æµç¨‹ï¼š

1. **è¨“ç·´å‰æ¸¬è©¦**ï¼šé©—è­‰è‡ªå®šç¾© token å·²æ­£ç¢ºæ·»åŠ åˆ° tokenizer
2. **è¨“ç·´ä¸­ç›£æ§**ï¼šè§€å¯Ÿå„å€‹å®¢å®¶è©±è…”èª¿çš„æå¤±å‡½æ•¸è®ŠåŒ–
3. **æ¨ç†æ¸¬è©¦**ï¼šæ¸¬è©¦æ¨¡å‹èƒ½å¦æ­£ç¢ºè¾¨è­˜ä¸åŒå®¢å®¶è©±è…”èª¿
4. **æ•ˆèƒ½è©•ä¼°**ï¼šæ¯”è¼ƒå¾®èª¿å‰å¾Œåœ¨å„è…”èª¿ä¸Šçš„è¡¨ç¾

## æˆåŠŸæ¨™æº–

- [x] è§£æ±º "Unsupported language" éŒ¯èª¤
- [x] ä¿æŒå®¢å®¶è©±è…”èª¿çš„ç¨ç‰¹æ€§å’Œå¯å€åˆ†æ€§  
- [x] æˆåŠŸæ·»åŠ è‡ªå®šç¾©èªè¨€ token åˆ° tokenizer
- [x] å¯¦ç¾å¤šèªè¨€è¨“ç·´è€Œä¸ä¸Ÿå¤±èªè¨€ç‰¹å¾µ

é€™å€‹è§£æ±ºæ–¹æ¡ˆæ—¢ä¿æŒäº† Whisper çš„å¤šèªè¨€èƒ½åŠ›ï¼Œåˆèƒ½ç²¾ç¢ºå€åˆ†ä¸åŒçš„å®¢å®¶è©±è…”èª¿ï¼Œç‚ºä½è³‡æºèªè¨€çš„èªéŸ³è­˜åˆ¥ç ”ç©¶æä¾›äº†å¯¦ç”¨çš„æŠ€è¡“è·¯å¾‘ã€‚