import argparse
import functools
import os

from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizerFast,\
    WhisperProcessor
from peft import PeftModel, PeftConfig
from utils.utils import print_arguments, add_arguments

parser = argparse.ArgumentParser(description=__doc__)
add_arg = functools.partial(add_arguments, argparser=parser)
add_arg("lora_model", type=str, default="output/whisper-tiny/checkpoint-best/", help="å¾®è°ƒä¿å­˜çš„æ¨¡å‹è·¯å¾„")
add_arg('output_dir', type=str, default='models/',    help="åˆå¹¶æ¨¡å‹çš„ä¿å­˜ç›®å½•")
add_arg("local_files_only", type=bool, default=False, help="æ˜¯å¦åªåœ¨æœ¬åœ°åŠ è½½æ¨¡å‹ï¼Œä¸å°è¯•ä¸‹è½½")
args = parser.parse_args()
print_arguments(args)

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
assert os.path.exists(args.lora_model), f"æ¨¡å‹æ–‡ä»¶{args.lora_model}ä¸å­˜åœ¨"
# è·å–Loraé…ç½®å‚æ•°
peft_config = PeftConfig.from_pretrained(args.lora_model)
# è·å–Whisperçš„åŸºæœ¬æ¨¡å‹
base_model = WhisperForConditionalGeneration.from_pretrained(peft_config.base_model_name_or_path, device_map={"": "cpu"},
                                                             local_files_only=args.local_files_only)

# è½½å…¥åŸºç¡€æ¨¡å‹çš„ tokenizer
tokenizer = WhisperTokenizerFast.from_pretrained(peft_config.base_model_name_or_path,
                                                 local_files_only=args.local_files_only)

# æ·»åŠ å¾®è°ƒæ—¶ä½¿ç”¨çš„å®¢å®¶è¯è¯­è¨€ tokensï¼ˆä¸ utils/reader.py ä¸­çš„è®¾ç½®ä¸€è‡´ï¼‰
hakka_languages = {
    'hakka_sixian': '<|hakka_sixian|>',
    'hakka_hailu': '<|hakka_hailu|>',
    'hakka_dapu': '<|hakka_dapu|>',
    'hakka_raoping': '<|hakka_raoping|>',
    'hakka_zhaoan': '<|hakka_zhaoan|>',
    'hakka_nansixian': '<|hakka_nansixian|>'
}

print(f"ğŸ”§ é‡æ–°åˆ›å»ºå¾®è°ƒæ—¶çš„ tokenizer è®¾ç½®")
new_tokens = []
vocab = tokenizer.get_vocab()

for lang_code, token in hakka_languages.items():
    if token not in vocab:
        new_tokens.append(token)
        print(f"   â• æ·»åŠ è¯­è¨€ tokenï¼š{token}")

if new_tokens:
    # æ·»åŠ ç‰¹æ®Š token
    tokenizer.add_special_tokens({"additional_special_tokens": new_tokens})
    print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_tokens)} ä¸ªè¯­è¨€ token")

# è°ƒæ•´æ¨¡å‹ä»¥é€‚åº”æ‰©å±•çš„è¯æ±‡è¡¨
if len(tokenizer.get_vocab()) > base_model.config.vocab_size:
    print(f"ğŸ“ˆ è¯æ±‡è¡¨å·²æ‰©å±•ï¼š{base_model.config.vocab_size} -> {len(tokenizer.get_vocab())}")
    print("ğŸ”§ è°ƒæ•´æ¨¡å‹ embedding å±‚å¤§å°...")
    
    # è°ƒæ•´æ¨¡å‹çš„ embedding å±‚
    base_model.resize_token_embeddings(len(tokenizer.get_vocab()))
    
    # æ›´æ–°æ¨¡å‹é…ç½®
    base_model.config.vocab_size = len(tokenizer.get_vocab())
    
    print(f"âœ… æ¨¡å‹ embedding å±‚å·²è°ƒæ•´ä¸º {base_model.config.vocab_size} tokens")

# ä¸Loraæ¨¡å‹åˆå¹¶
model = PeftModel.from_pretrained(base_model, args.lora_model, local_files_only=args.local_files_only)
feature_extractor = WhisperFeatureExtractor.from_pretrained(peft_config.base_model_name_or_path,
                                                            local_files_only=args.local_files_only)
processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path,
                                             local_files_only=args.local_files_only)

# åˆå¹¶å‚æ•°
model = model.merge_and_unload()
model.train(False)

# ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
if peft_config.base_model_name_or_path.endswith("/"):
    peft_config.base_model_name_or_path = peft_config.base_model_name_or_path[:-1]
save_directory = os.path.join(args.output_dir, f'{os.path.basename(peft_config.base_model_name_or_path)}-finetune')
os.makedirs(save_directory, exist_ok=True)

# ä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šç›®å½•ä¸­
model.save_pretrained(save_directory, max_shard_size='4GB')
feature_extractor.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)
processor.save_pretrained(save_directory)
print(f'åˆå¹¶æ¨¡å‹ä¿æŒåœ¨ï¼š{save_directory}')
